{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Тема «POS-tagger и NER»\n",
        "\n",
        "Задание 1. Написать теггер на данных с русским языком\n",
        "1. проверить UnigramTagger, BigramTagger, TrigramTagger и их комбинации\n",
        "2. написать свой теггер как на занятии, попробовать разные векторайзеры, добавить знание не только букв но и слов\n",
        "3. сравнить все реализованные методы, сделать выводы  \n",
        "\n",
        "\n",
        "Задание 2. Проверить, насколько хорошо работает NER\n",
        "Данные брать из Index of /pub/named_entities\n",
        "1. проверить NER из nltk/spacy/deeppavlov.\n",
        "2. написать свой NER, попробовать разные подходы.\n",
        "  - передаём в сетку токен и его соседей.\n",
        "  - передаём в сетку только токен.\n",
        "  - свой вариант.\n",
        "3. сравнить свои реализованные подходы на качество — вывести precision/recall/f1_score."
      ],
      "metadata": {
        "id": "9bHD9vo-KfLk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellId": "0b0ezez6ga0tclrxy67bqp",
        "id": "bf94c87d",
        "outputId": "79aae52d-d27b-42f1-e966-90e0cdce3841",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyconll\n",
            "  Downloading pyconll-3.2.0-py3-none-any.whl (27 kB)\n",
            "Installing collected packages: pyconll\n",
            "Successfully installed pyconll-3.2.0\n",
            "Collecting corus\n",
            "  Downloading corus-0.10.0-py3-none-any.whl (83 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.7/83.7 kB\u001b[0m \u001b[31m361.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: corus\n",
            "Successfully installed corus-0.10.0\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.5.4)\n",
            "Collecting spacy\n",
            "  Downloading spacy-3.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.9)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.22.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.27.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.0)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.3)\n",
            "Installing collected packages: spacy\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.5.4\n",
            "    Uninstalling spacy-3.5.4:\n",
            "      Successfully uninstalled spacy-3.5.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "en-core-web-sm 3.5.0 requires spacy<3.6.0,>=3.5.0, but you have spacy 3.6.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed spacy-3.6.0\n",
            "Collecting slovnet\n",
            "  Downloading slovnet-0.6.0-py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.7/46.7 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from slovnet) (1.22.4)\n",
            "Collecting razdel (from slovnet)\n",
            "  Downloading razdel-0.5.0-py3-none-any.whl (21 kB)\n",
            "Collecting navec (from slovnet)\n",
            "  Downloading navec-0.10.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: razdel, navec, slovnet\n",
            "Successfully installed navec-0.10.0 razdel-0.5.0 slovnet-0.6.0\n",
            "Collecting deeppavlov\n",
            "  Downloading deeppavlov-1.2.0-py3-none-any.whl (468 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.3/468.3 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi<=0.89.1,>=0.47.0 (from deeppavlov)\n",
            "  Downloading fastapi-0.89.1-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting filelock<3.10.0,>=3.0.0 (from deeppavlov)\n",
            "  Downloading filelock-3.9.1-py3-none-any.whl (9.7 kB)\n",
            "Requirement already satisfied: nltk<3.10.0,>=3.2.4 in /usr/local/lib/python3.10/dist-packages (from deeppavlov) (3.8.1)\n",
            "Requirement already satisfied: numpy<1.24 in /usr/local/lib/python3.10/dist-packages (from deeppavlov) (1.22.4)\n",
            "Requirement already satisfied: pandas<1.6.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from deeppavlov) (1.5.3)\n",
            "Requirement already satisfied: prometheus-client<=1.16.0,>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from deeppavlov) (0.17.1)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from deeppavlov) (1.10.12)\n",
            "Collecting pybind11==2.10.3 (from deeppavlov)\n",
            "  Downloading pybind11-2.10.3-py3-none-any.whl (222 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m222.4/222.4 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from deeppavlov) (2.27.1)\n",
            "Collecting scikit-learn<1.1.0,>=0.24 (from deeppavlov)\n",
            "  Downloading scikit_learn-1.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.5/26.5 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.10.0 (from deeppavlov)\n",
            "  Downloading scipy-1.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.7/33.7 MB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tqdm<4.65.0,>=4.42.0 (from deeppavlov)\n",
            "  Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn<0.19.0,>=0.13.0 (from deeppavlov)\n",
            "  Downloading uvicorn-0.18.3-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.4/57.4 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting starlette==0.22.0 (from fastapi<=0.89.1,>=0.47.0->deeppavlov)\n",
            "  Downloading starlette-0.22.0-py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.3/64.3 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette==0.22.0->fastapi<=0.89.1,>=0.47.0->deeppavlov) (3.7.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<3.10.0,>=3.2.4->deeppavlov) (8.1.6)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<3.10.0,>=3.2.4->deeppavlov) (1.3.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<3.10.0,>=3.2.4->deeppavlov) (2022.10.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas<1.6.0,>=1.0.0->deeppavlov) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<1.6.0,>=1.0.0->deeppavlov) (2022.7.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->deeppavlov) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.19.0->deeppavlov) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.19.0->deeppavlov) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.19.0->deeppavlov) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.19.0->deeppavlov) (3.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<1.1.0,>=0.24->deeppavlov) (3.2.0)\n",
            "Collecting h11>=0.8 (from uvicorn<0.19.0,>=0.13.0->deeppavlov)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas<1.6.0,>=1.0.0->deeppavlov) (1.16.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette==0.22.0->fastapi<=0.89.1,>=0.47.0->deeppavlov) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette==0.22.0->fastapi<=0.89.1,>=0.47.0->deeppavlov) (1.1.2)\n",
            "Installing collected packages: tqdm, scipy, pybind11, h11, filelock, uvicorn, starlette, scikit-learn, fastapi, deeppavlov\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.65.0\n",
            "    Uninstalling tqdm-4.65.0:\n",
            "      Successfully uninstalled tqdm-4.65.0\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.10.1\n",
            "    Uninstalling scipy-1.10.1:\n",
            "      Successfully uninstalled scipy-1.10.1\n",
            "  Attempting uninstall: filelock\n",
            "    Found existing installation: filelock 3.12.2\n",
            "    Uninstalling filelock-3.12.2:\n",
            "      Successfully uninstalled filelock-3.12.2\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "Successfully installed deeppavlov-1.2.0 fastapi-0.89.1 filelock-3.9.1 h11-0.14.0 pybind11-2.10.3 scikit-learn-1.0.2 scipy-1.9.3 starlette-0.22.0 tqdm-4.64.1 uvicorn-0.18.3\n",
            "Collecting ipymarkup\n",
            "  Downloading ipymarkup-0.9.0-py3-none-any.whl (14 kB)\n",
            "Collecting intervaltree>=3 (from ipymarkup)\n",
            "  Downloading intervaltree-3.1.0.tar.gz (32 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: sortedcontainers<3.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from intervaltree>=3->ipymarkup) (2.4.0)\n",
            "Building wheels for collected packages: intervaltree\n",
            "  Building wheel for intervaltree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for intervaltree: filename=intervaltree-3.1.0-py2.py3-none-any.whl size=26099 sha256=8c358b735909dd29b4fa925a56d536d9a114b11dd892d273cc5552997161eb55\n",
            "  Stored in directory: /root/.cache/pip/wheels/fa/80/8c/43488a924a046b733b64de3fac99252674c892a4c3801c0a61\n",
            "Successfully built intervaltree\n",
            "Installing collected packages: intervaltree, ipymarkup\n",
            "Successfully installed intervaltree-3.1.0 ipymarkup-0.9.0\n",
            "Collecting sklearn_crfsuite\n",
            "  Downloading sklearn_crfsuite-0.3.6-py2.py3-none-any.whl (12 kB)\n",
            "Collecting python-crfsuite>=0.8.3 (from sklearn_crfsuite)\n",
            "  Downloading python_crfsuite-0.9.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (993 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m993.5/993.5 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sklearn_crfsuite) (1.16.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from sklearn_crfsuite) (0.9.0)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.10/dist-packages (from sklearn_crfsuite) (4.64.1)\n",
            "Installing collected packages: python-crfsuite, sklearn_crfsuite\n",
            "Successfully installed python-crfsuite-0.9.9 sklearn_crfsuite-0.3.6\n"
          ]
        }
      ],
      "source": [
        "%pip install pyconll\n",
        "%pip install corus\n",
        "%pip install -U spacy\n",
        "%pip install slovnet\n",
        "%pip install deeppavlov\n",
        "%pip install ipymarkup\n",
        "%pip install sklearn_crfsuite"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir datasets\n",
        "!wget https://www.labinform.ru/pub/named_entities/collection5.zip\n",
        "!unzip collection5.zip -d ./datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6cLt9lZQkK6",
        "outputId": "a92f0d22-def7-46e5-f52b-8a6c69bcc919"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-08-04 12:29:21--  https://www.labinform.ru/pub/named_entities/collection5.zip\n",
            "Resolving www.labinform.ru (www.labinform.ru)... 95.181.230.181\n",
            "Connecting to www.labinform.ru (www.labinform.ru)|95.181.230.181|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1899530 (1.8M) [application/zip]\n",
            "Saving to: ‘collection5.zip’\n",
            "\n",
            "collection5.zip     100%[===================>]   1.81M  1.26MB/s    in 1.4s    \n",
            "\n",
            "2023-08-04 12:29:24 (1.26 MB/s) - ‘collection5.zip’ saved [1899530/1899530]\n",
            "\n",
            "Archive:  collection5.zip\n",
            "   creating: ./datasets/Collection5/\n",
            "  inflating: ./datasets/Collection5/001.ann  \n",
            "  inflating: ./datasets/Collection5/001.txt  \n",
            "  inflating: ./datasets/Collection5/002.ann  \n",
            "  inflating: ./datasets/Collection5/002.txt  \n",
            "  inflating: ./datasets/Collection5/003.ann  \n",
            "  inflating: ./datasets/Collection5/003.txt  \n",
            "  inflating: ./datasets/Collection5/004.ann  \n",
            "  inflating: ./datasets/Collection5/004.txt  \n",
            "  inflating: ./datasets/Collection5/005.ann  \n",
            "  inflating: ./datasets/Collection5/005.txt  \n",
            "  inflating: ./datasets/Collection5/006.ann  \n",
            "  inflating: ./datasets/Collection5/006.txt  \n",
            "  inflating: ./datasets/Collection5/007.ann  \n",
            "  inflating: ./datasets/Collection5/007.txt  \n",
            "  inflating: ./datasets/Collection5/008.ann  \n",
            "  inflating: ./datasets/Collection5/008.txt  \n",
            "  inflating: ./datasets/Collection5/009.ann  \n",
            "  inflating: ./datasets/Collection5/009.txt  \n",
            "  inflating: ./datasets/Collection5/010.ann  \n",
            "  inflating: ./datasets/Collection5/010.txt  \n",
            "  inflating: ./datasets/Collection5/011.ann  \n",
            "  inflating: ./datasets/Collection5/011.txt  \n",
            "  inflating: ./datasets/Collection5/012.ann  \n",
            "  inflating: ./datasets/Collection5/012.txt  \n",
            "  inflating: ./datasets/Collection5/013.ann  \n",
            "  inflating: ./datasets/Collection5/013.txt  \n",
            "  inflating: ./datasets/Collection5/014.ann  \n",
            "  inflating: ./datasets/Collection5/014.txt  \n",
            "  inflating: ./datasets/Collection5/015 (!).ann  \n",
            "  inflating: ./datasets/Collection5/015 (!).txt  \n",
            "  inflating: ./datasets/Collection5/016.ann  \n",
            "  inflating: ./datasets/Collection5/016.txt  \n",
            "  inflating: ./datasets/Collection5/017.ann  \n",
            "  inflating: ./datasets/Collection5/017.txt  \n",
            "  inflating: ./datasets/Collection5/018.ann  \n",
            "  inflating: ./datasets/Collection5/018.txt  \n",
            "  inflating: ./datasets/Collection5/019.ann  \n",
            "  inflating: ./datasets/Collection5/019.txt  \n",
            "  inflating: ./datasets/Collection5/020.ann  \n",
            "  inflating: ./datasets/Collection5/020.txt  \n",
            "  inflating: ./datasets/Collection5/021.ann  \n",
            "  inflating: ./datasets/Collection5/021.txt  \n",
            "  inflating: ./datasets/Collection5/022.ann  \n",
            "  inflating: ./datasets/Collection5/022.txt  \n",
            "  inflating: ./datasets/Collection5/023.ann  \n",
            "  inflating: ./datasets/Collection5/023.txt  \n",
            "  inflating: ./datasets/Collection5/025.ann  \n",
            "  inflating: ./datasets/Collection5/025.txt  \n",
            "  inflating: ./datasets/Collection5/026.ann  \n",
            "  inflating: ./datasets/Collection5/026.txt  \n",
            "  inflating: ./datasets/Collection5/027.ann  \n",
            "  inflating: ./datasets/Collection5/027.txt  \n",
            "  inflating: ./datasets/Collection5/028.ann  \n",
            "  inflating: ./datasets/Collection5/028.txt  \n",
            "  inflating: ./datasets/Collection5/029.ann  \n",
            "  inflating: ./datasets/Collection5/029.txt  \n",
            "  inflating: ./datasets/Collection5/030.ann  \n",
            "  inflating: ./datasets/Collection5/030.txt  \n",
            "  inflating: ./datasets/Collection5/031.ann  \n",
            "  inflating: ./datasets/Collection5/031.txt  \n",
            "  inflating: ./datasets/Collection5/032.ann  \n",
            "  inflating: ./datasets/Collection5/032.txt  \n",
            "  inflating: ./datasets/Collection5/033.ann  \n",
            "  inflating: ./datasets/Collection5/033.txt  \n",
            "  inflating: ./datasets/Collection5/034.ann  \n",
            "  inflating: ./datasets/Collection5/034.txt  \n",
            "  inflating: ./datasets/Collection5/035.ann  \n",
            "  inflating: ./datasets/Collection5/035.txt  \n",
            "  inflating: ./datasets/Collection5/036.ann  \n",
            "  inflating: ./datasets/Collection5/036.txt  \n",
            "  inflating: ./datasets/Collection5/037.ann  \n",
            "  inflating: ./datasets/Collection5/037.txt  \n",
            "  inflating: ./datasets/Collection5/038.ann  \n",
            "  inflating: ./datasets/Collection5/038.txt  \n",
            "  inflating: ./datasets/Collection5/039.ann  \n",
            "  inflating: ./datasets/Collection5/039.txt  \n",
            "  inflating: ./datasets/Collection5/03_12_12a.ann  \n",
            "  inflating: ./datasets/Collection5/03_12_12a.txt  \n",
            "  inflating: ./datasets/Collection5/03_12_12b.ann  \n",
            "  inflating: ./datasets/Collection5/03_12_12b.txt  \n",
            "  inflating: ./datasets/Collection5/03_12_12c.ann  \n",
            "  inflating: ./datasets/Collection5/03_12_12c.txt  \n",
            "  inflating: ./datasets/Collection5/03_12_12d.ann  \n",
            "  inflating: ./datasets/Collection5/03_12_12d.txt  \n",
            "  inflating: ./datasets/Collection5/03_12_12g.ann  \n",
            "  inflating: ./datasets/Collection5/03_12_12g.txt  \n",
            "  inflating: ./datasets/Collection5/03_12_12h.ann  \n",
            "  inflating: ./datasets/Collection5/03_12_12h.txt  \n",
            "  inflating: ./datasets/Collection5/040.ann  \n",
            "  inflating: ./datasets/Collection5/040.txt  \n",
            "  inflating: ./datasets/Collection5/041.ann  \n",
            "  inflating: ./datasets/Collection5/041.txt  \n",
            "  inflating: ./datasets/Collection5/042.ann  \n",
            "  inflating: ./datasets/Collection5/042.txt  \n",
            "  inflating: ./datasets/Collection5/043.ann  \n",
            "  inflating: ./datasets/Collection5/043.txt  \n",
            "  inflating: ./datasets/Collection5/044.ann  \n",
            "  inflating: ./datasets/Collection5/044.txt  \n",
            "  inflating: ./datasets/Collection5/045.ann  \n",
            "  inflating: ./datasets/Collection5/045.txt  \n",
            "  inflating: ./datasets/Collection5/046.ann  \n",
            "  inflating: ./datasets/Collection5/046.txt  \n",
            "  inflating: ./datasets/Collection5/047.ann  \n",
            "  inflating: ./datasets/Collection5/047.txt  \n",
            "  inflating: ./datasets/Collection5/048.ann  \n",
            "  inflating: ./datasets/Collection5/048.txt  \n",
            "  inflating: ./datasets/Collection5/049.ann  \n",
            "  inflating: ./datasets/Collection5/049.txt  \n",
            "  inflating: ./datasets/Collection5/04_02_13a_abdulatipov.ann  \n",
            "  inflating: ./datasets/Collection5/04_02_13a_abdulatipov.txt  \n",
            "  inflating: ./datasets/Collection5/04_03_13a_sorokin.ann  \n",
            "  inflating: ./datasets/Collection5/04_03_13a_sorokin.txt  \n",
            "  inflating: ./datasets/Collection5/04_12_12b.ann  \n",
            "  inflating: ./datasets/Collection5/04_12_12b.txt  \n",
            "  inflating: ./datasets/Collection5/04_12_12d.ann  \n",
            "  inflating: ./datasets/Collection5/04_12_12d.txt  \n",
            "  inflating: ./datasets/Collection5/04_12_12f.ann  \n",
            "  inflating: ./datasets/Collection5/04_12_12f.txt  \n",
            "  inflating: ./datasets/Collection5/04_12_12g.ann  \n",
            "  inflating: ./datasets/Collection5/04_12_12g.txt  \n",
            "  inflating: ./datasets/Collection5/04_12_12h_corr.ann  \n",
            "  inflating: ./datasets/Collection5/04_12_12h_corr.txt  \n",
            "  inflating: ./datasets/Collection5/050.ann  \n",
            "  inflating: ./datasets/Collection5/050.txt  \n",
            "  inflating: ./datasets/Collection5/051.ann  \n",
            "  inflating: ./datasets/Collection5/051.txt  \n",
            "  inflating: ./datasets/Collection5/052.ann  \n",
            "  inflating: ./datasets/Collection5/052.txt  \n",
            "  inflating: ./datasets/Collection5/053.ann  \n",
            "  inflating: ./datasets/Collection5/053.txt  \n",
            "  inflating: ./datasets/Collection5/054.ann  \n",
            "  inflating: ./datasets/Collection5/054.txt  \n",
            "  inflating: ./datasets/Collection5/055.ann  \n",
            "  inflating: ./datasets/Collection5/055.txt  \n",
            "  inflating: ./datasets/Collection5/056.ann  \n",
            "  inflating: ./datasets/Collection5/056.txt  \n",
            "  inflating: ./datasets/Collection5/057.ann  \n",
            "  inflating: ./datasets/Collection5/057.txt  \n",
            "  inflating: ./datasets/Collection5/058.ann  \n",
            "  inflating: ./datasets/Collection5/058.txt  \n",
            "  inflating: ./datasets/Collection5/059.ann  \n",
            "  inflating: ./datasets/Collection5/059.txt  \n",
            "  inflating: ./datasets/Collection5/060.ann  \n",
            "  inflating: ./datasets/Collection5/060.txt  \n",
            "  inflating: ./datasets/Collection5/061.ann  \n",
            "  inflating: ./datasets/Collection5/061.txt  \n",
            "  inflating: ./datasets/Collection5/062.ann  \n",
            "  inflating: ./datasets/Collection5/062.txt  \n",
            "  inflating: ./datasets/Collection5/063.ann  \n",
            "  inflating: ./datasets/Collection5/063.txt  \n",
            "  inflating: ./datasets/Collection5/064.ann  \n",
            "  inflating: ./datasets/Collection5/064.txt  \n",
            "  inflating: ./datasets/Collection5/065.ann  \n",
            "  inflating: ./datasets/Collection5/065.txt  \n",
            "  inflating: ./datasets/Collection5/066.ann  \n",
            "  inflating: ./datasets/Collection5/066.txt  \n",
            "  inflating: ./datasets/Collection5/067.ann  \n",
            "  inflating: ./datasets/Collection5/067.txt  \n",
            "  inflating: ./datasets/Collection5/068.ann  \n",
            "  inflating: ./datasets/Collection5/068.txt  \n",
            "  inflating: ./datasets/Collection5/069.ann  \n",
            "  inflating: ./datasets/Collection5/069.txt  \n",
            "  inflating: ./datasets/Collection5/070.ann  \n",
            "  inflating: ./datasets/Collection5/070.txt  \n",
            "  inflating: ./datasets/Collection5/071.ann  \n",
            "  inflating: ./datasets/Collection5/071.txt  \n",
            "  inflating: ./datasets/Collection5/072.ann  \n",
            "  inflating: ./datasets/Collection5/072.txt  \n",
            "  inflating: ./datasets/Collection5/073.ann  \n",
            "  inflating: ./datasets/Collection5/073.txt  \n",
            "  inflating: ./datasets/Collection5/074.ann  \n",
            "  inflating: ./datasets/Collection5/074.txt  \n",
            "  inflating: ./datasets/Collection5/075.ann  \n",
            "  inflating: ./datasets/Collection5/075.txt  \n",
            "  inflating: ./datasets/Collection5/076.ann  \n",
            "  inflating: ./datasets/Collection5/076.txt  \n",
            "  inflating: ./datasets/Collection5/077.ann  \n",
            "  inflating: ./datasets/Collection5/077.txt  \n",
            "  inflating: ./datasets/Collection5/078.ann  \n",
            "  inflating: ./datasets/Collection5/078.txt  \n",
            "  inflating: ./datasets/Collection5/079.ann  \n",
            "  inflating: ./datasets/Collection5/079.txt  \n",
            "  inflating: ./datasets/Collection5/080.ann  \n",
            "  inflating: ./datasets/Collection5/080.txt  \n",
            "  inflating: ./datasets/Collection5/081.ann  \n",
            "  inflating: ./datasets/Collection5/081.txt  \n",
            "  inflating: ./datasets/Collection5/082.ann  \n",
            "  inflating: ./datasets/Collection5/082.txt  \n",
            "  inflating: ./datasets/Collection5/083.ann  \n",
            "  inflating: ./datasets/Collection5/083.txt  \n",
            "  inflating: ./datasets/Collection5/084.ann  \n",
            "  inflating: ./datasets/Collection5/084.txt  \n",
            "  inflating: ./datasets/Collection5/085.ann  \n",
            "  inflating: ./datasets/Collection5/085.txt  \n",
            "  inflating: ./datasets/Collection5/086.ann  \n",
            "  inflating: ./datasets/Collection5/086.txt  \n",
            "  inflating: ./datasets/Collection5/087.ann  \n",
            "  inflating: ./datasets/Collection5/087.txt  \n",
            "  inflating: ./datasets/Collection5/088.ann  \n",
            "  inflating: ./datasets/Collection5/088.txt  \n",
            "  inflating: ./datasets/Collection5/089.ann  \n",
            "  inflating: ./datasets/Collection5/089.txt  \n",
            "  inflating: ./datasets/Collection5/090.ann  \n",
            "  inflating: ./datasets/Collection5/090.txt  \n",
            "  inflating: ./datasets/Collection5/091.ann  \n",
            "  inflating: ./datasets/Collection5/091.txt  \n",
            "  inflating: ./datasets/Collection5/092.ann  \n",
            "  inflating: ./datasets/Collection5/092.txt  \n",
            "  inflating: ./datasets/Collection5/093.ann  \n",
            "  inflating: ./datasets/Collection5/093.txt  \n",
            "  inflating: ./datasets/Collection5/094.ann  \n",
            "  inflating: ./datasets/Collection5/094.txt  \n",
            "  inflating: ./datasets/Collection5/095.ann  \n",
            "  inflating: ./datasets/Collection5/095.txt  \n",
            "  inflating: ./datasets/Collection5/096.ann  \n",
            "  inflating: ./datasets/Collection5/096.txt  \n",
            "  inflating: ./datasets/Collection5/097.ann  \n",
            "  inflating: ./datasets/Collection5/097.txt  \n",
            "  inflating: ./datasets/Collection5/098.ann  \n",
            "  inflating: ./datasets/Collection5/098.txt  \n",
            "  inflating: ./datasets/Collection5/099.ann  \n",
            "  inflating: ./datasets/Collection5/099.txt  \n",
            "  inflating: ./datasets/Collection5/09_01_13.ann  \n",
            "  inflating: ./datasets/Collection5/09_01_13.txt  \n",
            "  inflating: ./datasets/Collection5/09_01_13a.ann  \n",
            "  inflating: ./datasets/Collection5/09_01_13a.txt  \n",
            "  inflating: ./datasets/Collection5/09_01_13c.ann  \n",
            "  inflating: ./datasets/Collection5/09_01_13c.txt  \n",
            "  inflating: ./datasets/Collection5/09_01_13d.ann  \n",
            "  inflating: ./datasets/Collection5/09_01_13d.txt  \n",
            "  inflating: ./datasets/Collection5/09_01_13e.ann  \n",
            "  inflating: ./datasets/Collection5/09_01_13e.txt  \n",
            "  inflating: ./datasets/Collection5/09_01_13h.ann  \n",
            "  inflating: ./datasets/Collection5/09_01_13h.txt  \n",
            "  inflating: ./datasets/Collection5/09_01_13i.ann  \n",
            "  inflating: ./datasets/Collection5/09_01_13i.txt  \n",
            "  inflating: ./datasets/Collection5/100.ann  \n",
            "  inflating: ./datasets/Collection5/100.txt  \n",
            "  inflating: ./datasets/Collection5/1000.ann  \n",
            "  inflating: ./datasets/Collection5/1000.txt  \n",
            "  inflating: ./datasets/Collection5/1001.ann  \n",
            "  inflating: ./datasets/Collection5/1001.txt  \n",
            "  inflating: ./datasets/Collection5/1002.ann  \n",
            "  inflating: ./datasets/Collection5/1002.txt  \n",
            "  inflating: ./datasets/Collection5/1003.ann  \n",
            "  inflating: ./datasets/Collection5/1003.txt  \n",
            "  inflating: ./datasets/Collection5/1004.ann  \n",
            "  inflating: ./datasets/Collection5/1004.txt  \n",
            "  inflating: ./datasets/Collection5/1005.ann  \n",
            "  inflating: ./datasets/Collection5/1005.txt  \n",
            "  inflating: ./datasets/Collection5/1006.ann  \n",
            "  inflating: ./datasets/Collection5/1006.txt  \n",
            "  inflating: ./datasets/Collection5/1007.ann  \n",
            "  inflating: ./datasets/Collection5/1007.txt  \n",
            "  inflating: ./datasets/Collection5/1008.ann  \n",
            "  inflating: ./datasets/Collection5/1008.txt  \n",
            "  inflating: ./datasets/Collection5/1009.ann  \n",
            "  inflating: ./datasets/Collection5/1009.txt  \n",
            "  inflating: ./datasets/Collection5/101.ann  \n",
            "  inflating: ./datasets/Collection5/101.txt  \n",
            "  inflating: ./datasets/Collection5/1010.ann  \n",
            "  inflating: ./datasets/Collection5/1010.txt  \n",
            "  inflating: ./datasets/Collection5/1011.ann  \n",
            "  inflating: ./datasets/Collection5/1011.txt  \n",
            "  inflating: ./datasets/Collection5/1012.ann  \n",
            "  inflating: ./datasets/Collection5/1012.txt  \n",
            "  inflating: ./datasets/Collection5/1013.ann  \n",
            "  inflating: ./datasets/Collection5/1013.txt  \n",
            "  inflating: ./datasets/Collection5/1014.ann  \n",
            "  inflating: ./datasets/Collection5/1014.txt  \n",
            "  inflating: ./datasets/Collection5/1015.ann  \n",
            "  inflating: ./datasets/Collection5/1015.txt  \n",
            "  inflating: ./datasets/Collection5/1016.ann  \n",
            "  inflating: ./datasets/Collection5/1016.txt  \n",
            "  inflating: ./datasets/Collection5/1017.ann  \n",
            "  inflating: ./datasets/Collection5/1017.txt  \n",
            "  inflating: ./datasets/Collection5/1018.ann  \n",
            "  inflating: ./datasets/Collection5/1018.txt  \n",
            "  inflating: ./datasets/Collection5/1019.ann  \n",
            "  inflating: ./datasets/Collection5/1019.txt  \n",
            "  inflating: ./datasets/Collection5/102.ann  \n",
            "  inflating: ./datasets/Collection5/102.txt  \n",
            "  inflating: ./datasets/Collection5/1020.ann  \n",
            "  inflating: ./datasets/Collection5/1020.txt  \n",
            "  inflating: ./datasets/Collection5/1021.ann  \n",
            "  inflating: ./datasets/Collection5/1021.txt  \n",
            "  inflating: ./datasets/Collection5/1022.ann  \n",
            "  inflating: ./datasets/Collection5/1022.txt  \n",
            "  inflating: ./datasets/Collection5/1023.ann  \n",
            "  inflating: ./datasets/Collection5/1023.txt  \n",
            "  inflating: ./datasets/Collection5/1024.ann  \n",
            "  inflating: ./datasets/Collection5/1024.txt  \n",
            "  inflating: ./datasets/Collection5/1025.ann  \n",
            "  inflating: ./datasets/Collection5/1025.txt  \n",
            "  inflating: ./datasets/Collection5/1026.ann  \n",
            "  inflating: ./datasets/Collection5/1026.txt  \n",
            "  inflating: ./datasets/Collection5/1027.ann  \n",
            "  inflating: ./datasets/Collection5/1027.txt  \n",
            "  inflating: ./datasets/Collection5/1028.ann  \n",
            "  inflating: ./datasets/Collection5/1028.txt  \n",
            "  inflating: ./datasets/Collection5/1029.ann  \n",
            "  inflating: ./datasets/Collection5/1029.txt  \n",
            "  inflating: ./datasets/Collection5/103.ann  \n",
            "  inflating: ./datasets/Collection5/103.txt  \n",
            "  inflating: ./datasets/Collection5/1030.ann  \n",
            "  inflating: ./datasets/Collection5/1030.txt  \n",
            "  inflating: ./datasets/Collection5/1031.ann  \n",
            "  inflating: ./datasets/Collection5/1031.txt  \n",
            "  inflating: ./datasets/Collection5/1032.ann  \n",
            "  inflating: ./datasets/Collection5/1032.txt  \n",
            "  inflating: ./datasets/Collection5/1033.ann  \n",
            "  inflating: ./datasets/Collection5/1033.txt  \n",
            "  inflating: ./datasets/Collection5/1034.ann  \n",
            "  inflating: ./datasets/Collection5/1034.txt  \n",
            "  inflating: ./datasets/Collection5/1035.ann  \n",
            "  inflating: ./datasets/Collection5/1035.txt  \n",
            "  inflating: ./datasets/Collection5/1036.ann  \n",
            "  inflating: ./datasets/Collection5/1036.txt  \n",
            "  inflating: ./datasets/Collection5/1037.ann  \n",
            "  inflating: ./datasets/Collection5/1037.txt  \n",
            "  inflating: ./datasets/Collection5/1038.ann  \n",
            "  inflating: ./datasets/Collection5/1038.txt  \n",
            "  inflating: ./datasets/Collection5/1039.ann  \n",
            "  inflating: ./datasets/Collection5/1039.txt  \n",
            "  inflating: ./datasets/Collection5/104.ann  \n",
            "  inflating: ./datasets/Collection5/104.txt  \n",
            "  inflating: ./datasets/Collection5/1040.ann  \n",
            "  inflating: ./datasets/Collection5/1040.txt  \n",
            "  inflating: ./datasets/Collection5/1041.ann  \n",
            "  inflating: ./datasets/Collection5/1041.txt  \n",
            "  inflating: ./datasets/Collection5/1042.ann  \n",
            "  inflating: ./datasets/Collection5/1042.txt  \n",
            "  inflating: ./datasets/Collection5/1043.ann  \n",
            "  inflating: ./datasets/Collection5/1043.txt  \n",
            "  inflating: ./datasets/Collection5/1044.ann  \n",
            "  inflating: ./datasets/Collection5/1044.txt  \n",
            "  inflating: ./datasets/Collection5/1045.ann  \n",
            "  inflating: ./datasets/Collection5/1045.txt  \n",
            "  inflating: ./datasets/Collection5/1046.ann  \n",
            "  inflating: ./datasets/Collection5/1046.txt  \n",
            "  inflating: ./datasets/Collection5/1047.ann  \n",
            "  inflating: ./datasets/Collection5/1047.txt  \n",
            "  inflating: ./datasets/Collection5/1048.ann  \n",
            "  inflating: ./datasets/Collection5/1048.txt  \n",
            "  inflating: ./datasets/Collection5/1049.ann  \n",
            "  inflating: ./datasets/Collection5/1049.txt  \n",
            "  inflating: ./datasets/Collection5/105.ann  \n",
            "  inflating: ./datasets/Collection5/105.txt  \n",
            "  inflating: ./datasets/Collection5/1050.ann  \n",
            "  inflating: ./datasets/Collection5/1050.txt  \n",
            "  inflating: ./datasets/Collection5/106.ann  \n",
            "  inflating: ./datasets/Collection5/106.txt  \n",
            "  inflating: ./datasets/Collection5/107.ann  \n",
            "  inflating: ./datasets/Collection5/107.txt  \n",
            "  inflating: ./datasets/Collection5/108.ann  \n",
            "  inflating: ./datasets/Collection5/108.txt  \n",
            "  inflating: ./datasets/Collection5/109.ann  \n",
            "  inflating: ./datasets/Collection5/109.txt  \n",
            "  inflating: ./datasets/Collection5/10_01_13a.ann  \n",
            "  inflating: ./datasets/Collection5/10_01_13a.txt  \n",
            "  inflating: ./datasets/Collection5/10_01_13d.ann  \n",
            "  inflating: ./datasets/Collection5/10_01_13d.txt  \n",
            "  inflating: ./datasets/Collection5/10_01_13i.ann  \n",
            "  inflating: ./datasets/Collection5/10_01_13i.txt  \n",
            "  inflating: ./datasets/Collection5/110.ann  \n",
            "  inflating: ./datasets/Collection5/110.txt  \n",
            "  inflating: ./datasets/Collection5/1100.ann  \n",
            "  inflating: ./datasets/Collection5/1100.txt  \n",
            "  inflating: ./datasets/Collection5/1101.ann  \n",
            "  inflating: ./datasets/Collection5/1101.txt  \n",
            "  inflating: ./datasets/Collection5/1102.ann  \n",
            "  inflating: ./datasets/Collection5/1102.txt  \n",
            "  inflating: ./datasets/Collection5/1103.ann  \n",
            "  inflating: ./datasets/Collection5/1103.txt  \n",
            "  inflating: ./datasets/Collection5/1104.ann  \n",
            "  inflating: ./datasets/Collection5/1104.txt  \n",
            "  inflating: ./datasets/Collection5/1105.ann  \n",
            "  inflating: ./datasets/Collection5/1105.txt  \n",
            "  inflating: ./datasets/Collection5/1106.ann  \n",
            "  inflating: ./datasets/Collection5/1106.txt  \n",
            "  inflating: ./datasets/Collection5/1107.ann  \n",
            "  inflating: ./datasets/Collection5/1107.txt  \n",
            "  inflating: ./datasets/Collection5/1108.ann  \n",
            "  inflating: ./datasets/Collection5/1108.txt  \n",
            "  inflating: ./datasets/Collection5/1109.ann  \n",
            "  inflating: ./datasets/Collection5/1109.txt  \n",
            "  inflating: ./datasets/Collection5/111.ann  \n",
            "  inflating: ./datasets/Collection5/111.txt  \n",
            "  inflating: ./datasets/Collection5/1110.ann  \n",
            "  inflating: ./datasets/Collection5/1110.txt  \n",
            "  inflating: ./datasets/Collection5/1111.ann  \n",
            "  inflating: ./datasets/Collection5/1111.txt  \n",
            "  inflating: ./datasets/Collection5/1112.ann  \n",
            "  inflating: ./datasets/Collection5/1112.txt  \n",
            "  inflating: ./datasets/Collection5/1113.ann  \n",
            "  inflating: ./datasets/Collection5/1113.txt  \n",
            "  inflating: ./datasets/Collection5/1114.ann  \n",
            "  inflating: ./datasets/Collection5/1114.txt  \n",
            "  inflating: ./datasets/Collection5/1115.ann  \n",
            "  inflating: ./datasets/Collection5/1115.txt  \n",
            "  inflating: ./datasets/Collection5/1116.ann  \n",
            "  inflating: ./datasets/Collection5/1116.txt  \n",
            "  inflating: ./datasets/Collection5/1117.ann  \n",
            "  inflating: ./datasets/Collection5/1117.txt  \n",
            "  inflating: ./datasets/Collection5/1118.ann  \n",
            "  inflating: ./datasets/Collection5/1118.txt  \n",
            "  inflating: ./datasets/Collection5/1119.ann  \n",
            "  inflating: ./datasets/Collection5/1119.txt  \n",
            "  inflating: ./datasets/Collection5/112.ann  \n",
            "  inflating: ./datasets/Collection5/112.txt  \n",
            "  inflating: ./datasets/Collection5/1120.ann  \n",
            "  inflating: ./datasets/Collection5/1120.txt  \n",
            "  inflating: ./datasets/Collection5/1121.ann  \n",
            "  inflating: ./datasets/Collection5/1121.txt  \n",
            "  inflating: ./datasets/Collection5/1122.ann  \n",
            "  inflating: ./datasets/Collection5/1122.txt  \n",
            "  inflating: ./datasets/Collection5/1123.ann  \n",
            "  inflating: ./datasets/Collection5/1123.txt  \n",
            "  inflating: ./datasets/Collection5/1124.ann  \n",
            "  inflating: ./datasets/Collection5/1124.txt  \n",
            "  inflating: ./datasets/Collection5/1125.ann  \n",
            "  inflating: ./datasets/Collection5/1125.txt  \n",
            "  inflating: ./datasets/Collection5/1126.ann  \n",
            "  inflating: ./datasets/Collection5/1126.txt  \n",
            "  inflating: ./datasets/Collection5/1127.ann  \n",
            "  inflating: ./datasets/Collection5/1127.txt  \n",
            "  inflating: ./datasets/Collection5/1128.ann  \n",
            "  inflating: ./datasets/Collection5/1128.txt  \n",
            "  inflating: ./datasets/Collection5/113.ann  \n",
            "  inflating: ./datasets/Collection5/113.txt  \n",
            "  inflating: ./datasets/Collection5/1130.ann  \n",
            "  inflating: ./datasets/Collection5/1130.txt  \n",
            "  inflating: ./datasets/Collection5/1131.ann  \n",
            "  inflating: ./datasets/Collection5/1131.txt  \n",
            "  inflating: ./datasets/Collection5/1132.ann  \n",
            "  inflating: ./datasets/Collection5/1132.txt  \n",
            "  inflating: ./datasets/Collection5/1133.ann  \n",
            "  inflating: ./datasets/Collection5/1133.txt  \n",
            "  inflating: ./datasets/Collection5/1134.ann  \n",
            "  inflating: ./datasets/Collection5/1134.txt  \n",
            "  inflating: ./datasets/Collection5/1135.ann  \n",
            "  inflating: ./datasets/Collection5/1135.txt  \n",
            "  inflating: ./datasets/Collection5/1136.ann  \n",
            "  inflating: ./datasets/Collection5/1136.txt  \n",
            "  inflating: ./datasets/Collection5/1137.ann  \n",
            "  inflating: ./datasets/Collection5/1137.txt  \n",
            "  inflating: ./datasets/Collection5/1138.ann  \n",
            "  inflating: ./datasets/Collection5/1138.txt  \n",
            "  inflating: ./datasets/Collection5/1139.ann  \n",
            "  inflating: ./datasets/Collection5/1139.txt  \n",
            "  inflating: ./datasets/Collection5/114.ann  \n",
            "  inflating: ./datasets/Collection5/114.txt  \n",
            "  inflating: ./datasets/Collection5/1140.ann  \n",
            "  inflating: ./datasets/Collection5/1140.txt  \n",
            "  inflating: ./datasets/Collection5/1141.ann  \n",
            "  inflating: ./datasets/Collection5/1141.txt  \n",
            "  inflating: ./datasets/Collection5/1142.ann  \n",
            "  inflating: ./datasets/Collection5/1142.txt  \n",
            "  inflating: ./datasets/Collection5/1143.ann  \n",
            "  inflating: ./datasets/Collection5/1143.txt  \n",
            "  inflating: ./datasets/Collection5/1144.ann  \n",
            "  inflating: ./datasets/Collection5/1144.txt  \n",
            "  inflating: ./datasets/Collection5/1145.ann  \n",
            "  inflating: ./datasets/Collection5/1145.txt  \n",
            "  inflating: ./datasets/Collection5/1146.ann  \n",
            "  inflating: ./datasets/Collection5/1146.txt  \n",
            "  inflating: ./datasets/Collection5/1147.ann  \n",
            "  inflating: ./datasets/Collection5/1147.txt  \n",
            "  inflating: ./datasets/Collection5/1148.ann  \n",
            "  inflating: ./datasets/Collection5/1148.txt  \n",
            "  inflating: ./datasets/Collection5/1149.ann  \n",
            "  inflating: ./datasets/Collection5/1149.txt  \n",
            "  inflating: ./datasets/Collection5/115.ann  \n",
            "  inflating: ./datasets/Collection5/115.txt  \n",
            "  inflating: ./datasets/Collection5/1150.ann  \n",
            "  inflating: ./datasets/Collection5/1150.txt  \n",
            "  inflating: ./datasets/Collection5/1151.ann  \n",
            "  inflating: ./datasets/Collection5/1151.txt  \n",
            "  inflating: ./datasets/Collection5/1152.ann  \n",
            "  inflating: ./datasets/Collection5/1152.txt  \n",
            "  inflating: ./datasets/Collection5/1153.ann  \n",
            "  inflating: ./datasets/Collection5/1153.txt  \n",
            "  inflating: ./datasets/Collection5/1154.ann  \n",
            "  inflating: ./datasets/Collection5/1154.txt  \n",
            "  inflating: ./datasets/Collection5/1155.ann  \n",
            "  inflating: ./datasets/Collection5/1155.txt  \n",
            "  inflating: ./datasets/Collection5/1156.ann  \n",
            "  inflating: ./datasets/Collection5/1156.txt  \n",
            "  inflating: ./datasets/Collection5/1157.ann  \n",
            "  inflating: ./datasets/Collection5/1157.txt  \n",
            "  inflating: ./datasets/Collection5/1158.ann  \n",
            "  inflating: ./datasets/Collection5/1158.txt  \n",
            "  inflating: ./datasets/Collection5/1159.ann  \n",
            "  inflating: ./datasets/Collection5/1159.txt  \n",
            "  inflating: ./datasets/Collection5/116.ann  \n",
            "  inflating: ./datasets/Collection5/116.txt  \n",
            "  inflating: ./datasets/Collection5/1160.ann  \n",
            "  inflating: ./datasets/Collection5/1160.txt  \n",
            "  inflating: ./datasets/Collection5/1161.ann  \n",
            "  inflating: ./datasets/Collection5/1161.txt  \n",
            "  inflating: ./datasets/Collection5/1162.ann  \n",
            "  inflating: ./datasets/Collection5/1162.txt  \n",
            "  inflating: ./datasets/Collection5/1163.ann  \n",
            "  inflating: ./datasets/Collection5/1163.txt  \n",
            "  inflating: ./datasets/Collection5/1164.ann  \n",
            "  inflating: ./datasets/Collection5/1164.txt  \n",
            "  inflating: ./datasets/Collection5/1165.ann  \n",
            "  inflating: ./datasets/Collection5/1165.txt  \n",
            "  inflating: ./datasets/Collection5/1166.ann  \n",
            "  inflating: ./datasets/Collection5/1166.txt  \n",
            "  inflating: ./datasets/Collection5/1167.ann  \n",
            "  inflating: ./datasets/Collection5/1167.txt  \n",
            "  inflating: ./datasets/Collection5/1168.ann  \n",
            "  inflating: ./datasets/Collection5/1168.txt  \n",
            "  inflating: ./datasets/Collection5/1169.ann  \n",
            "  inflating: ./datasets/Collection5/1169.txt  \n",
            "  inflating: ./datasets/Collection5/117.ann  \n",
            "  inflating: ./datasets/Collection5/117.txt  \n",
            "  inflating: ./datasets/Collection5/1170.ann  \n",
            "  inflating: ./datasets/Collection5/1170.txt  \n",
            "  inflating: ./datasets/Collection5/1171.ann  \n",
            "  inflating: ./datasets/Collection5/1171.txt  \n",
            "  inflating: ./datasets/Collection5/1172.ann  \n",
            "  inflating: ./datasets/Collection5/1172.txt  \n",
            "  inflating: ./datasets/Collection5/1173.ann  \n",
            "  inflating: ./datasets/Collection5/1173.txt  \n",
            "  inflating: ./datasets/Collection5/1174.ann  \n",
            "  inflating: ./datasets/Collection5/1174.txt  \n",
            "  inflating: ./datasets/Collection5/1175.ann  \n",
            "  inflating: ./datasets/Collection5/1175.txt  \n",
            "  inflating: ./datasets/Collection5/1176.ann  \n",
            "  inflating: ./datasets/Collection5/1176.txt  \n",
            "  inflating: ./datasets/Collection5/1177.ann  \n",
            "  inflating: ./datasets/Collection5/1177.txt  \n",
            "  inflating: ./datasets/Collection5/1178.ann  \n",
            "  inflating: ./datasets/Collection5/1178.txt  \n",
            "  inflating: ./datasets/Collection5/1179.ann  \n",
            "  inflating: ./datasets/Collection5/1179.txt  \n",
            "  inflating: ./datasets/Collection5/118.ann  \n",
            "  inflating: ./datasets/Collection5/118.txt  \n",
            "  inflating: ./datasets/Collection5/1180.ann  \n",
            "  inflating: ./datasets/Collection5/1180.txt  \n",
            "  inflating: ./datasets/Collection5/1181.ann  \n",
            "  inflating: ./datasets/Collection5/1181.txt  \n",
            "  inflating: ./datasets/Collection5/1182.ann  \n",
            "  inflating: ./datasets/Collection5/1182.txt  \n",
            "  inflating: ./datasets/Collection5/1183.ann  \n",
            "  inflating: ./datasets/Collection5/1183.txt  \n",
            "  inflating: ./datasets/Collection5/1184.ann  \n",
            "  inflating: ./datasets/Collection5/1184.txt  \n",
            "  inflating: ./datasets/Collection5/1185.ann  \n",
            "  inflating: ./datasets/Collection5/1185.txt  \n",
            "  inflating: ./datasets/Collection5/1186.ann  \n",
            "  inflating: ./datasets/Collection5/1186.txt  \n",
            "  inflating: ./datasets/Collection5/1187.ann  \n",
            "  inflating: ./datasets/Collection5/1187.txt  \n",
            "  inflating: ./datasets/Collection5/1188.ann  \n",
            "  inflating: ./datasets/Collection5/1188.txt  \n",
            "  inflating: ./datasets/Collection5/1189.ann  \n",
            "  inflating: ./datasets/Collection5/1189.txt  \n",
            "  inflating: ./datasets/Collection5/119.ann  \n",
            "  inflating: ./datasets/Collection5/119.txt  \n",
            "  inflating: ./datasets/Collection5/1190.ann  \n",
            "  inflating: ./datasets/Collection5/1190.txt  \n",
            "  inflating: ./datasets/Collection5/1191.ann  \n",
            "  inflating: ./datasets/Collection5/1191.txt  \n",
            "  inflating: ./datasets/Collection5/1192.ann  \n",
            "  inflating: ./datasets/Collection5/1192.txt  \n",
            "  inflating: ./datasets/Collection5/1193.ann  \n",
            "  inflating: ./datasets/Collection5/1193.txt  \n",
            "  inflating: ./datasets/Collection5/1194.ann  \n",
            "  inflating: ./datasets/Collection5/1194.txt  \n",
            "  inflating: ./datasets/Collection5/1195.ann  \n",
            "  inflating: ./datasets/Collection5/1195.txt  \n",
            "  inflating: ./datasets/Collection5/1196.ann  \n",
            "  inflating: ./datasets/Collection5/1196.txt  \n",
            "  inflating: ./datasets/Collection5/1197.ann  \n",
            "  inflating: ./datasets/Collection5/1197.txt  \n",
            "  inflating: ./datasets/Collection5/1198.ann  \n",
            "  inflating: ./datasets/Collection5/1198.txt  \n",
            "  inflating: ./datasets/Collection5/1199.ann  \n",
            "  inflating: ./datasets/Collection5/1199.txt  \n",
            "  inflating: ./datasets/Collection5/11_01_13b.ann  \n",
            "  inflating: ./datasets/Collection5/11_01_13b.txt  \n",
            "  inflating: ./datasets/Collection5/11_01_13e.ann  \n",
            "  inflating: ./datasets/Collection5/11_01_13e.txt  \n",
            "  inflating: ./datasets/Collection5/120.ann  \n",
            "  inflating: ./datasets/Collection5/120.txt  \n",
            "  inflating: ./datasets/Collection5/1200.ann  \n",
            "  inflating: ./datasets/Collection5/1200.txt  \n",
            "  inflating: ./datasets/Collection5/121.ann  \n",
            "  inflating: ./datasets/Collection5/121.txt  \n",
            "  inflating: ./datasets/Collection5/122.ann  \n",
            "  inflating: ./datasets/Collection5/122.txt  \n",
            "  inflating: ./datasets/Collection5/123.ann  \n",
            "  inflating: ./datasets/Collection5/123.txt  \n",
            "  inflating: ./datasets/Collection5/124.ann  \n",
            "  inflating: ./datasets/Collection5/124.txt  \n",
            "  inflating: ./datasets/Collection5/125.ann  \n",
            "  inflating: ./datasets/Collection5/125.txt  \n",
            "  inflating: ./datasets/Collection5/126.ann  \n",
            "  inflating: ./datasets/Collection5/126.txt  \n",
            "  inflating: ./datasets/Collection5/127.ann  \n",
            "  inflating: ./datasets/Collection5/127.txt  \n",
            "  inflating: ./datasets/Collection5/128.ann  \n",
            "  inflating: ./datasets/Collection5/128.txt  \n",
            "  inflating: ./datasets/Collection5/129.ann  \n",
            "  inflating: ./datasets/Collection5/129.txt  \n",
            "  inflating: ./datasets/Collection5/130.ann  \n",
            "  inflating: ./datasets/Collection5/130.txt  \n",
            "  inflating: ./datasets/Collection5/131.ann  \n",
            "  inflating: ./datasets/Collection5/131.txt  \n",
            "  inflating: ./datasets/Collection5/132.ann  \n",
            "  inflating: ./datasets/Collection5/132.txt  \n",
            "  inflating: ./datasets/Collection5/133.ann  \n",
            "  inflating: ./datasets/Collection5/133.txt  \n",
            "  inflating: ./datasets/Collection5/134.ann  \n",
            "  inflating: ./datasets/Collection5/134.txt  \n",
            "  inflating: ./datasets/Collection5/135.ann  \n",
            "  inflating: ./datasets/Collection5/135.txt  \n",
            "  inflating: ./datasets/Collection5/136.ann  \n",
            "  inflating: ./datasets/Collection5/136.txt  \n",
            "  inflating: ./datasets/Collection5/137.ann  \n",
            "  inflating: ./datasets/Collection5/137.txt  \n",
            "  inflating: ./datasets/Collection5/138.ann  \n",
            "  inflating: ./datasets/Collection5/138.txt  \n",
            "  inflating: ./datasets/Collection5/139.ann  \n",
            "  inflating: ./datasets/Collection5/139.txt  \n",
            "  inflating: ./datasets/Collection5/140.ann  \n",
            "  inflating: ./datasets/Collection5/140.txt  \n",
            "  inflating: ./datasets/Collection5/141.ann  \n",
            "  inflating: ./datasets/Collection5/141.txt  \n",
            "  inflating: ./datasets/Collection5/142.ann  \n",
            "  inflating: ./datasets/Collection5/142.txt  \n",
            "  inflating: ./datasets/Collection5/143.ann  \n",
            "  inflating: ./datasets/Collection5/143.txt  \n",
            "  inflating: ./datasets/Collection5/144.ann  \n",
            "  inflating: ./datasets/Collection5/144.txt  \n",
            "  inflating: ./datasets/Collection5/145.ann  \n",
            "  inflating: ./datasets/Collection5/145.txt  \n",
            "  inflating: ./datasets/Collection5/146.ann  \n",
            "  inflating: ./datasets/Collection5/146.txt  \n",
            "  inflating: ./datasets/Collection5/147.ann  \n",
            "  inflating: ./datasets/Collection5/147.txt  \n",
            "  inflating: ./datasets/Collection5/148.ann  \n",
            "  inflating: ./datasets/Collection5/148.txt  \n",
            "  inflating: ./datasets/Collection5/149.ann  \n",
            "  inflating: ./datasets/Collection5/149.txt  \n",
            "  inflating: ./datasets/Collection5/14_01_13c.ann  \n",
            "  inflating: ./datasets/Collection5/14_01_13c.txt  \n",
            "  inflating: ./datasets/Collection5/14_01_13g.ann  \n",
            "  inflating: ./datasets/Collection5/14_01_13g.txt  \n",
            "  inflating: ./datasets/Collection5/14_01_13i.ann  \n",
            "  inflating: ./datasets/Collection5/14_01_13i.txt  \n",
            "  inflating: ./datasets/Collection5/150.ann  \n",
            "  inflating: ./datasets/Collection5/150.txt  \n",
            "  inflating: ./datasets/Collection5/151.ann  \n",
            "  inflating: ./datasets/Collection5/151.txt  \n",
            "  inflating: ./datasets/Collection5/152.ann  \n",
            "  inflating: ./datasets/Collection5/152.txt  \n",
            "  inflating: ./datasets/Collection5/153.ann  \n",
            "  inflating: ./datasets/Collection5/153.txt  \n",
            "  inflating: ./datasets/Collection5/154.ann  \n",
            "  inflating: ./datasets/Collection5/154.txt  \n",
            "  inflating: ./datasets/Collection5/155.ann  \n",
            "  inflating: ./datasets/Collection5/155.txt  \n",
            "  inflating: ./datasets/Collection5/156.ann  \n",
            "  inflating: ./datasets/Collection5/156.txt  \n",
            "  inflating: ./datasets/Collection5/157.ann  \n",
            "  inflating: ./datasets/Collection5/157.txt  \n",
            "  inflating: ./datasets/Collection5/158.ann  \n",
            "  inflating: ./datasets/Collection5/158.txt  \n",
            "  inflating: ./datasets/Collection5/159.ann  \n",
            "  inflating: ./datasets/Collection5/159.txt  \n",
            "  inflating: ./datasets/Collection5/15_01_13a.ann  \n",
            "  inflating: ./datasets/Collection5/15_01_13a.txt  \n",
            "  inflating: ./datasets/Collection5/15_01_13b.ann  \n",
            "  inflating: ./datasets/Collection5/15_01_13b.txt  \n",
            "  inflating: ./datasets/Collection5/15_01_13e.ann  \n",
            "  inflating: ./datasets/Collection5/15_01_13e.txt  \n",
            "  inflating: ./datasets/Collection5/15_01_13f.ann  \n",
            "  inflating: ./datasets/Collection5/15_01_13f.txt  \n",
            "  inflating: ./datasets/Collection5/160.ann  \n",
            "  inflating: ./datasets/Collection5/160.txt  \n",
            "  inflating: ./datasets/Collection5/161.ann  \n",
            "  inflating: ./datasets/Collection5/161.txt  \n",
            "  inflating: ./datasets/Collection5/162.ann  \n",
            "  inflating: ./datasets/Collection5/162.txt  \n",
            "  inflating: ./datasets/Collection5/163.ann  \n",
            "  inflating: ./datasets/Collection5/163.txt  \n",
            "  inflating: ./datasets/Collection5/164.ann  \n",
            "  inflating: ./datasets/Collection5/164.txt  \n",
            "  inflating: ./datasets/Collection5/165.ann  \n",
            "  inflating: ./datasets/Collection5/165.txt  \n",
            "  inflating: ./datasets/Collection5/166.ann  \n",
            "  inflating: ./datasets/Collection5/166.txt  \n",
            "  inflating: ./datasets/Collection5/167.ann  \n",
            "  inflating: ./datasets/Collection5/167.txt  \n",
            "  inflating: ./datasets/Collection5/168.ann  \n",
            "  inflating: ./datasets/Collection5/168.txt  \n",
            "  inflating: ./datasets/Collection5/169.ann  \n",
            "  inflating: ./datasets/Collection5/169.txt  \n",
            "  inflating: ./datasets/Collection5/170.ann  \n",
            "  inflating: ./datasets/Collection5/170.txt  \n",
            "  inflating: ./datasets/Collection5/171.ann  \n",
            "  inflating: ./datasets/Collection5/171.txt  \n",
            "  inflating: ./datasets/Collection5/172.ann  \n",
            "  inflating: ./datasets/Collection5/172.txt  \n",
            "  inflating: ./datasets/Collection5/173.ann  \n",
            "  inflating: ./datasets/Collection5/173.txt  \n",
            "  inflating: ./datasets/Collection5/174.ann  \n",
            "  inflating: ./datasets/Collection5/174.txt  \n",
            "  inflating: ./datasets/Collection5/175.ann  \n",
            "  inflating: ./datasets/Collection5/175.txt  \n",
            "  inflating: ./datasets/Collection5/176.ann  \n",
            "  inflating: ./datasets/Collection5/176.txt  \n",
            "  inflating: ./datasets/Collection5/177.ann  \n",
            "  inflating: ./datasets/Collection5/177.txt  \n",
            "  inflating: ./datasets/Collection5/178.ann  \n",
            "  inflating: ./datasets/Collection5/178.txt  \n",
            "  inflating: ./datasets/Collection5/179.ann  \n",
            "  inflating: ./datasets/Collection5/179.txt  \n",
            "  inflating: ./datasets/Collection5/180.ann  \n",
            "  inflating: ./datasets/Collection5/180.txt  \n",
            "  inflating: ./datasets/Collection5/181.ann  \n",
            "  inflating: ./datasets/Collection5/181.txt  \n",
            "  inflating: ./datasets/Collection5/182.ann  \n",
            "  inflating: ./datasets/Collection5/182.txt  \n",
            "  inflating: ./datasets/Collection5/183.ann  \n",
            "  inflating: ./datasets/Collection5/183.txt  \n",
            "  inflating: ./datasets/Collection5/184.ann  \n",
            "  inflating: ./datasets/Collection5/184.txt  \n",
            "  inflating: ./datasets/Collection5/185.ann  \n",
            "  inflating: ./datasets/Collection5/185.txt  \n",
            "  inflating: ./datasets/Collection5/186.ann  \n",
            "  inflating: ./datasets/Collection5/186.txt  \n",
            "  inflating: ./datasets/Collection5/187.ann  \n",
            "  inflating: ./datasets/Collection5/187.txt  \n",
            "  inflating: ./datasets/Collection5/188.ann  \n",
            "  inflating: ./datasets/Collection5/188.txt  \n",
            "  inflating: ./datasets/Collection5/189.ann  \n",
            "  inflating: ./datasets/Collection5/189.txt  \n",
            "  inflating: ./datasets/Collection5/190.ann  \n",
            "  inflating: ./datasets/Collection5/190.txt  \n",
            "  inflating: ./datasets/Collection5/191.ann  \n",
            "  inflating: ./datasets/Collection5/191.txt  \n",
            "  inflating: ./datasets/Collection5/192.ann  \n",
            "  inflating: ./datasets/Collection5/192.txt  \n",
            "  inflating: ./datasets/Collection5/193.ann  \n",
            "  inflating: ./datasets/Collection5/193.txt  \n",
            "  inflating: ./datasets/Collection5/194.ann  \n",
            "  inflating: ./datasets/Collection5/194.txt  \n",
            "  inflating: ./datasets/Collection5/195.ann  \n",
            "  inflating: ./datasets/Collection5/195.txt  \n",
            "  inflating: ./datasets/Collection5/196.ann  \n",
            "  inflating: ./datasets/Collection5/196.txt  \n",
            "  inflating: ./datasets/Collection5/197.ann  \n",
            "  inflating: ./datasets/Collection5/197.txt  \n",
            "  inflating: ./datasets/Collection5/198.ann  \n",
            "  inflating: ./datasets/Collection5/198.txt  \n",
            "  inflating: ./datasets/Collection5/199.ann  \n",
            "  inflating: ./datasets/Collection5/199.txt  \n",
            "  inflating: ./datasets/Collection5/19_11_12d.ann  \n",
            "  inflating: ./datasets/Collection5/19_11_12d.txt  \n",
            "  inflating: ./datasets/Collection5/19_11_12h.ann  \n",
            "  inflating: ./datasets/Collection5/19_11_12h.txt  \n",
            "  inflating: ./datasets/Collection5/200.ann  \n",
            "  inflating: ./datasets/Collection5/200.txt  \n",
            "  inflating: ./datasets/Collection5/2001.ann  \n",
            "  inflating: ./datasets/Collection5/2001.txt  \n",
            "  inflating: ./datasets/Collection5/2002.ann  \n",
            "  inflating: ./datasets/Collection5/2002.txt  \n",
            "  inflating: ./datasets/Collection5/2003.ann  \n",
            "  inflating: ./datasets/Collection5/2003.txt  \n",
            "  inflating: ./datasets/Collection5/2004.ann  \n",
            "  inflating: ./datasets/Collection5/2004.txt  \n",
            "  inflating: ./datasets/Collection5/2005.ann  \n",
            "  inflating: ./datasets/Collection5/2005.txt  \n",
            "  inflating: ./datasets/Collection5/2006.ann  \n",
            "  inflating: ./datasets/Collection5/2006.txt  \n",
            "  inflating: ./datasets/Collection5/2007.ann  \n",
            "  inflating: ./datasets/Collection5/2007.txt  \n",
            "  inflating: ./datasets/Collection5/2008.ann  \n",
            "  inflating: ./datasets/Collection5/2008.txt  \n",
            "  inflating: ./datasets/Collection5/2009.ann  \n",
            "  inflating: ./datasets/Collection5/2009.txt  \n",
            "  inflating: ./datasets/Collection5/201.ann  \n",
            "  inflating: ./datasets/Collection5/201.txt  \n",
            "  inflating: ./datasets/Collection5/2010.ann  \n",
            "  inflating: ./datasets/Collection5/2010.txt  \n",
            "  inflating: ./datasets/Collection5/2011.ann  \n",
            "  inflating: ./datasets/Collection5/2011.txt  \n",
            "  inflating: ./datasets/Collection5/2012.ann  \n",
            "  inflating: ./datasets/Collection5/2012.txt  \n",
            "  inflating: ./datasets/Collection5/2013.ann  \n",
            "  inflating: ./datasets/Collection5/2013.txt  \n",
            "  inflating: ./datasets/Collection5/2014.ann  \n",
            "  inflating: ./datasets/Collection5/2014.txt  \n",
            "  inflating: ./datasets/Collection5/2015.ann  \n",
            "  inflating: ./datasets/Collection5/2015.txt  \n",
            "  inflating: ./datasets/Collection5/2016.ann  \n",
            "  inflating: ./datasets/Collection5/2016.txt  \n",
            "  inflating: ./datasets/Collection5/2017.ann  \n",
            "  inflating: ./datasets/Collection5/2017.txt  \n",
            "  inflating: ./datasets/Collection5/2018.ann  \n",
            "  inflating: ./datasets/Collection5/2018.txt  \n",
            "  inflating: ./datasets/Collection5/2019.ann  \n",
            "  inflating: ./datasets/Collection5/2019.txt  \n",
            "  inflating: ./datasets/Collection5/202.ann  \n",
            "  inflating: ./datasets/Collection5/202.txt  \n",
            "  inflating: ./datasets/Collection5/2020.ann  \n",
            "  inflating: ./datasets/Collection5/2020.txt  \n",
            "  inflating: ./datasets/Collection5/2021.ann  \n",
            "  inflating: ./datasets/Collection5/2021.txt  \n",
            "  inflating: ./datasets/Collection5/2022.ann  \n",
            "  inflating: ./datasets/Collection5/2022.txt  \n",
            "  inflating: ./datasets/Collection5/2023.ann  \n",
            "  inflating: ./datasets/Collection5/2023.txt  \n",
            "  inflating: ./datasets/Collection5/2024.ann  \n",
            "  inflating: ./datasets/Collection5/2024.txt  \n",
            "  inflating: ./datasets/Collection5/2025.ann  \n",
            "  inflating: ./datasets/Collection5/2025.txt  \n",
            "  inflating: ./datasets/Collection5/2026.ann  \n",
            "  inflating: ./datasets/Collection5/2026.txt  \n",
            "  inflating: ./datasets/Collection5/2027.ann  \n",
            "  inflating: ./datasets/Collection5/2027.txt  \n",
            "  inflating: ./datasets/Collection5/2028.ann  \n",
            "  inflating: ./datasets/Collection5/2028.txt  \n",
            "  inflating: ./datasets/Collection5/2029.ann  \n",
            "  inflating: ./datasets/Collection5/2029.txt  \n",
            "  inflating: ./datasets/Collection5/203.ann  \n",
            "  inflating: ./datasets/Collection5/203.txt  \n",
            "  inflating: ./datasets/Collection5/2030.ann  \n",
            "  inflating: ./datasets/Collection5/2030.txt  \n",
            "  inflating: ./datasets/Collection5/2031.ann  \n",
            "  inflating: ./datasets/Collection5/2031.txt  \n",
            "  inflating: ./datasets/Collection5/2032.ann  \n",
            "  inflating: ./datasets/Collection5/2032.txt  \n",
            "  inflating: ./datasets/Collection5/2034.ann  \n",
            "  inflating: ./datasets/Collection5/2034.txt  \n",
            "  inflating: ./datasets/Collection5/2035.ann  \n",
            "  inflating: ./datasets/Collection5/2035.txt  \n",
            "  inflating: ./datasets/Collection5/2036.ann  \n",
            "  inflating: ./datasets/Collection5/2036.txt  \n",
            "  inflating: ./datasets/Collection5/2037.ann  \n",
            "  inflating: ./datasets/Collection5/2037.txt  \n",
            "  inflating: ./datasets/Collection5/2038.ann  \n",
            "  inflating: ./datasets/Collection5/2038.txt  \n",
            "  inflating: ./datasets/Collection5/2039.ann  \n",
            "  inflating: ./datasets/Collection5/2039.txt  \n",
            "  inflating: ./datasets/Collection5/204.ann  \n",
            "  inflating: ./datasets/Collection5/204.txt  \n",
            "  inflating: ./datasets/Collection5/2040.ann  \n",
            "  inflating: ./datasets/Collection5/2040.txt  \n",
            "  inflating: ./datasets/Collection5/2041.ann  \n",
            "  inflating: ./datasets/Collection5/2041.txt  \n",
            "  inflating: ./datasets/Collection5/2042.ann  \n",
            "  inflating: ./datasets/Collection5/2042.txt  \n",
            "  inflating: ./datasets/Collection5/2043.ann  \n",
            "  inflating: ./datasets/Collection5/2043.txt  \n",
            "  inflating: ./datasets/Collection5/2044.ann  \n",
            "  inflating: ./datasets/Collection5/2044.txt  \n",
            "  inflating: ./datasets/Collection5/2045.ann  \n",
            "  inflating: ./datasets/Collection5/2045.txt  \n",
            "  inflating: ./datasets/Collection5/2046.ann  \n",
            "  inflating: ./datasets/Collection5/2046.txt  \n",
            "  inflating: ./datasets/Collection5/2047.ann  \n",
            "  inflating: ./datasets/Collection5/2047.txt  \n",
            "  inflating: ./datasets/Collection5/2048.ann  \n",
            "  inflating: ./datasets/Collection5/2048.txt  \n",
            "  inflating: ./datasets/Collection5/2049.ann  \n",
            "  inflating: ./datasets/Collection5/2049.txt  \n",
            "  inflating: ./datasets/Collection5/205.ann  \n",
            "  inflating: ./datasets/Collection5/205.txt  \n",
            "  inflating: ./datasets/Collection5/2050.ann  \n",
            "  inflating: ./datasets/Collection5/2050.txt  \n",
            "  inflating: ./datasets/Collection5/206.ann  \n",
            "  inflating: ./datasets/Collection5/206.txt  \n",
            "  inflating: ./datasets/Collection5/207.ann  \n",
            "  inflating: ./datasets/Collection5/207.txt  \n",
            "  inflating: ./datasets/Collection5/208.ann  \n",
            "  inflating: ./datasets/Collection5/208.txt  \n",
            "  inflating: ./datasets/Collection5/209.ann  \n",
            "  inflating: ./datasets/Collection5/209.txt  \n",
            "  inflating: ./datasets/Collection5/20_11_12a.ann  \n",
            "  inflating: ./datasets/Collection5/20_11_12a.txt  \n",
            "  inflating: ./datasets/Collection5/20_11_12b.ann  \n",
            "  inflating: ./datasets/Collection5/20_11_12b.txt  \n",
            "  inflating: ./datasets/Collection5/20_11_12c.ann  \n",
            "  inflating: ./datasets/Collection5/20_11_12c.txt  \n",
            "  inflating: ./datasets/Collection5/20_11_12d.ann  \n",
            "  inflating: ./datasets/Collection5/20_11_12d.txt  \n",
            "  inflating: ./datasets/Collection5/20_11_12i.ann  \n",
            "  inflating: ./datasets/Collection5/20_11_12i.txt  \n",
            "  inflating: ./datasets/Collection5/210.ann  \n",
            "  inflating: ./datasets/Collection5/210.txt  \n",
            "  inflating: ./datasets/Collection5/211.ann  \n",
            "  inflating: ./datasets/Collection5/211.txt  \n",
            "  inflating: ./datasets/Collection5/212.ann  \n",
            "  inflating: ./datasets/Collection5/212.txt  \n",
            "  inflating: ./datasets/Collection5/213.ann  \n",
            "  inflating: ./datasets/Collection5/213.txt  \n",
            "  inflating: ./datasets/Collection5/214.ann  \n",
            "  inflating: ./datasets/Collection5/214.txt  \n",
            "  inflating: ./datasets/Collection5/215.ann  \n",
            "  inflating: ./datasets/Collection5/215.txt  \n",
            "  inflating: ./datasets/Collection5/216.ann  \n",
            "  inflating: ./datasets/Collection5/216.txt  \n",
            "  inflating: ./datasets/Collection5/217.ann  \n",
            "  inflating: ./datasets/Collection5/217.txt  \n",
            "  inflating: ./datasets/Collection5/218.ann  \n",
            "  inflating: ./datasets/Collection5/218.txt  \n",
            "  inflating: ./datasets/Collection5/219.ann  \n",
            "  inflating: ./datasets/Collection5/219.txt  \n",
            "  inflating: ./datasets/Collection5/21_11_12c.ann  \n",
            "  inflating: ./datasets/Collection5/21_11_12c.txt  \n",
            "  inflating: ./datasets/Collection5/21_11_12h.ann  \n",
            "  inflating: ./datasets/Collection5/21_11_12h.txt  \n",
            "  inflating: ./datasets/Collection5/21_11_12i.ann  \n",
            "  inflating: ./datasets/Collection5/21_11_12i.txt  \n",
            "  inflating: ./datasets/Collection5/21_11_12j.ann  \n",
            "  inflating: ./datasets/Collection5/21_11_12j.txt  \n",
            "  inflating: ./datasets/Collection5/220.ann  \n",
            "  inflating: ./datasets/Collection5/220.txt  \n",
            "  inflating: ./datasets/Collection5/221.ann  \n",
            "  inflating: ./datasets/Collection5/221.txt  \n",
            "  inflating: ./datasets/Collection5/222.ann  \n",
            "  inflating: ./datasets/Collection5/222.txt  \n",
            "  inflating: ./datasets/Collection5/223.ann  \n",
            "  inflating: ./datasets/Collection5/223.txt  \n",
            "  inflating: ./datasets/Collection5/224.ann  \n",
            "  inflating: ./datasets/Collection5/224.txt  \n",
            "  inflating: ./datasets/Collection5/225.ann  \n",
            "  inflating: ./datasets/Collection5/225.txt  \n",
            "  inflating: ./datasets/Collection5/226.ann  \n",
            "  inflating: ./datasets/Collection5/226.txt  \n",
            "  inflating: ./datasets/Collection5/227.ann  \n",
            "  inflating: ./datasets/Collection5/227.txt  \n",
            "  inflating: ./datasets/Collection5/228.ann  \n",
            "  inflating: ./datasets/Collection5/228.txt  \n",
            "  inflating: ./datasets/Collection5/229.ann  \n",
            "  inflating: ./datasets/Collection5/229.txt  \n",
            "  inflating: ./datasets/Collection5/22_11_12a.ann  \n",
            "  inflating: ./datasets/Collection5/22_11_12a.txt  \n",
            "  inflating: ./datasets/Collection5/22_11_12c.ann  \n",
            "  inflating: ./datasets/Collection5/22_11_12c.txt  \n",
            "  inflating: ./datasets/Collection5/22_11_12d.ann  \n",
            "  inflating: ./datasets/Collection5/22_11_12d.txt  \n",
            "  inflating: ./datasets/Collection5/22_11_12g.ann  \n",
            "  inflating: ./datasets/Collection5/22_11_12g.txt  \n",
            "  inflating: ./datasets/Collection5/22_11_12h.ann  \n",
            "  inflating: ./datasets/Collection5/22_11_12h.txt  \n",
            "  inflating: ./datasets/Collection5/22_11_12i.ann  \n",
            "  inflating: ./datasets/Collection5/22_11_12i.txt  \n",
            "  inflating: ./datasets/Collection5/22_11_12j.ann  \n",
            "  inflating: ./datasets/Collection5/22_11_12j.txt  \n",
            "  inflating: ./datasets/Collection5/230.ann  \n",
            "  inflating: ./datasets/Collection5/230.txt  \n",
            "  inflating: ./datasets/Collection5/231.ann  \n",
            "  inflating: ./datasets/Collection5/231.txt  \n",
            "  inflating: ./datasets/Collection5/232.ann  \n",
            "  inflating: ./datasets/Collection5/232.txt  \n",
            "  inflating: ./datasets/Collection5/233.ann  \n",
            "  inflating: ./datasets/Collection5/233.txt  \n",
            "  inflating: ./datasets/Collection5/234.ann  \n",
            "  inflating: ./datasets/Collection5/234.txt  \n",
            "  inflating: ./datasets/Collection5/235.ann  \n",
            "  inflating: ./datasets/Collection5/235.txt  \n",
            "  inflating: ./datasets/Collection5/236.ann  \n",
            "  inflating: ./datasets/Collection5/236.txt  \n",
            "  inflating: ./datasets/Collection5/237.ann  \n",
            "  inflating: ./datasets/Collection5/237.txt  \n",
            "  inflating: ./datasets/Collection5/238.ann  \n",
            "  inflating: ./datasets/Collection5/238.txt  \n",
            "  inflating: ./datasets/Collection5/239.ann  \n",
            "  inflating: ./datasets/Collection5/239.txt  \n",
            "  inflating: ./datasets/Collection5/23_11_12a.ann  \n",
            "  inflating: ./datasets/Collection5/23_11_12a.txt  \n",
            "  inflating: ./datasets/Collection5/23_11_12b.ann  \n",
            "  inflating: ./datasets/Collection5/23_11_12b.txt  \n",
            "  inflating: ./datasets/Collection5/23_11_12c.ann  \n",
            "  inflating: ./datasets/Collection5/23_11_12c.txt  \n",
            "  inflating: ./datasets/Collection5/23_11_12d.ann  \n",
            "  inflating: ./datasets/Collection5/23_11_12d.txt  \n",
            "  inflating: ./datasets/Collection5/23_11_12e.ann  \n",
            "  inflating: ./datasets/Collection5/23_11_12e.txt  \n",
            "  inflating: ./datasets/Collection5/23_11_12f.ann  \n",
            "  inflating: ./datasets/Collection5/23_11_12f.txt  \n",
            "  inflating: ./datasets/Collection5/240.ann  \n",
            "  inflating: ./datasets/Collection5/240.txt  \n",
            "  inflating: ./datasets/Collection5/241.ann  \n",
            "  inflating: ./datasets/Collection5/241.txt  \n",
            "  inflating: ./datasets/Collection5/242.ann  \n",
            "  inflating: ./datasets/Collection5/242.txt  \n",
            "  inflating: ./datasets/Collection5/243.ann  \n",
            "  inflating: ./datasets/Collection5/243.txt  \n",
            "  inflating: ./datasets/Collection5/244.ann  \n",
            "  inflating: ./datasets/Collection5/244.txt  \n",
            "  inflating: ./datasets/Collection5/245.ann  \n",
            "  inflating: ./datasets/Collection5/245.txt  \n",
            "  inflating: ./datasets/Collection5/246.ann  \n",
            "  inflating: ./datasets/Collection5/246.txt  \n",
            "  inflating: ./datasets/Collection5/247.ann  \n",
            "  inflating: ./datasets/Collection5/247.txt  \n",
            "  inflating: ./datasets/Collection5/248.ann  \n",
            "  inflating: ./datasets/Collection5/248.txt  \n",
            "  inflating: ./datasets/Collection5/249.ann  \n",
            "  inflating: ./datasets/Collection5/249.txt  \n",
            "  inflating: ./datasets/Collection5/250.ann  \n",
            "  inflating: ./datasets/Collection5/250.txt  \n",
            "  inflating: ./datasets/Collection5/251.ann  \n",
            "  inflating: ./datasets/Collection5/251.txt  \n",
            "  inflating: ./datasets/Collection5/252.ann  \n",
            "  inflating: ./datasets/Collection5/252.txt  \n",
            "  inflating: ./datasets/Collection5/253.ann  \n",
            "  inflating: ./datasets/Collection5/253.txt  \n",
            "  inflating: ./datasets/Collection5/254.ann  \n",
            "  inflating: ./datasets/Collection5/254.txt  \n",
            "  inflating: ./datasets/Collection5/255.ann  \n",
            "  inflating: ./datasets/Collection5/255.txt  \n",
            "  inflating: ./datasets/Collection5/256.ann  \n",
            "  inflating: ./datasets/Collection5/256.txt  \n",
            "  inflating: ./datasets/Collection5/257.ann  \n",
            "  inflating: ./datasets/Collection5/257.txt  \n",
            "  inflating: ./datasets/Collection5/258.ann  \n",
            "  inflating: ./datasets/Collection5/258.txt  \n",
            "  inflating: ./datasets/Collection5/259.ann  \n",
            "  inflating: ./datasets/Collection5/259.txt  \n",
            "  inflating: ./datasets/Collection5/25_12_12a.ann  \n",
            "  inflating: ./datasets/Collection5/25_12_12a.txt  \n",
            "  inflating: ./datasets/Collection5/25_12_12c.ann  \n",
            "  inflating: ./datasets/Collection5/25_12_12c.txt  \n",
            "  inflating: ./datasets/Collection5/25_12_12d.ann  \n",
            "  inflating: ./datasets/Collection5/25_12_12d.txt  \n",
            "  inflating: ./datasets/Collection5/25_12_12e.ann  \n",
            "  inflating: ./datasets/Collection5/25_12_12e.txt  \n",
            "  inflating: ./datasets/Collection5/260.ann  \n",
            "  inflating: ./datasets/Collection5/260.txt  \n",
            "  inflating: ./datasets/Collection5/261.ann  \n",
            "  inflating: ./datasets/Collection5/261.txt  \n",
            "  inflating: ./datasets/Collection5/262.ann  \n",
            "  inflating: ./datasets/Collection5/262.txt  \n",
            "  inflating: ./datasets/Collection5/263.ann  \n",
            "  inflating: ./datasets/Collection5/263.txt  \n",
            "  inflating: ./datasets/Collection5/264.ann  \n",
            "  inflating: ./datasets/Collection5/264.txt  \n",
            "  inflating: ./datasets/Collection5/265.ann  \n",
            "  inflating: ./datasets/Collection5/265.txt  \n",
            "  inflating: ./datasets/Collection5/266.ann  \n",
            "  inflating: ./datasets/Collection5/266.txt  \n",
            "  inflating: ./datasets/Collection5/267.ann  \n",
            "  inflating: ./datasets/Collection5/267.txt  \n",
            "  inflating: ./datasets/Collection5/268.ann  \n",
            "  inflating: ./datasets/Collection5/268.txt  \n",
            "  inflating: ./datasets/Collection5/269.ann  \n",
            "  inflating: ./datasets/Collection5/269.txt  \n",
            "  inflating: ./datasets/Collection5/26_11_12b.ann  \n",
            "  inflating: ./datasets/Collection5/26_11_12b.txt  \n",
            "  inflating: ./datasets/Collection5/26_11_12c.ann  \n",
            "  inflating: ./datasets/Collection5/26_11_12c.txt  \n",
            "  inflating: ./datasets/Collection5/26_11_12e.ann  \n",
            "  inflating: ./datasets/Collection5/26_11_12e.txt  \n",
            "  inflating: ./datasets/Collection5/26_11_12f.ann  \n",
            "  inflating: ./datasets/Collection5/26_11_12f.txt  \n",
            "  inflating: ./datasets/Collection5/270.ann  \n",
            "  inflating: ./datasets/Collection5/270.txt  \n",
            "  inflating: ./datasets/Collection5/271.ann  \n",
            "  inflating: ./datasets/Collection5/271.txt  \n",
            "  inflating: ./datasets/Collection5/272.ann  \n",
            "  inflating: ./datasets/Collection5/272.txt  \n",
            "  inflating: ./datasets/Collection5/273.ann  \n",
            "  inflating: ./datasets/Collection5/273.txt  \n",
            "  inflating: ./datasets/Collection5/274.ann  \n",
            "  inflating: ./datasets/Collection5/274.txt  \n",
            "  inflating: ./datasets/Collection5/275.ann  \n",
            "  inflating: ./datasets/Collection5/275.txt  \n",
            "  inflating: ./datasets/Collection5/276.ann  \n",
            "  inflating: ./datasets/Collection5/276.txt  \n",
            "  inflating: ./datasets/Collection5/277.ann  \n",
            "  inflating: ./datasets/Collection5/277.txt  \n",
            "  inflating: ./datasets/Collection5/278.ann  \n",
            "  inflating: ./datasets/Collection5/278.txt  \n",
            "  inflating: ./datasets/Collection5/279.ann  \n",
            "  inflating: ./datasets/Collection5/279.txt  \n",
            "  inflating: ./datasets/Collection5/27_11_12a.ann  \n",
            "  inflating: ./datasets/Collection5/27_11_12a.txt  \n",
            "  inflating: ./datasets/Collection5/27_11_12c.ann  \n",
            "  inflating: ./datasets/Collection5/27_11_12c.txt  \n",
            "  inflating: ./datasets/Collection5/27_11_12d.ann  \n",
            "  inflating: ./datasets/Collection5/27_11_12d.txt  \n",
            "  inflating: ./datasets/Collection5/27_11_12e.ann  \n",
            "  inflating: ./datasets/Collection5/27_11_12e.txt  \n",
            "  inflating: ./datasets/Collection5/27_11_12j.ann  \n",
            "  inflating: ./datasets/Collection5/27_11_12j.txt  \n",
            "  inflating: ./datasets/Collection5/280.ann  \n",
            "  inflating: ./datasets/Collection5/280.txt  \n",
            "  inflating: ./datasets/Collection5/281.ann  \n",
            "  inflating: ./datasets/Collection5/281.txt  \n",
            "  inflating: ./datasets/Collection5/282.ann  \n",
            "  inflating: ./datasets/Collection5/282.txt  \n",
            "  inflating: ./datasets/Collection5/283.ann  \n",
            "  inflating: ./datasets/Collection5/283.txt  \n",
            "  inflating: ./datasets/Collection5/284.ann  \n",
            "  inflating: ./datasets/Collection5/284.txt  \n",
            "  inflating: ./datasets/Collection5/285.ann  \n",
            "  inflating: ./datasets/Collection5/285.txt  \n",
            "  inflating: ./datasets/Collection5/286.ann  \n",
            "  inflating: ./datasets/Collection5/286.txt  \n",
            "  inflating: ./datasets/Collection5/287.ann  \n",
            "  inflating: ./datasets/Collection5/287.txt  \n",
            "  inflating: ./datasets/Collection5/288.ann  \n",
            "  inflating: ./datasets/Collection5/288.txt  \n",
            "  inflating: ./datasets/Collection5/289.ann  \n",
            "  inflating: ./datasets/Collection5/289.txt  \n",
            "  inflating: ./datasets/Collection5/28_11_12a.ann  \n",
            "  inflating: ./datasets/Collection5/28_11_12a.txt  \n",
            "  inflating: ./datasets/Collection5/28_11_12f.ann  \n",
            "  inflating: ./datasets/Collection5/28_11_12f.txt  \n",
            "  inflating: ./datasets/Collection5/28_11_12g.ann  \n",
            "  inflating: ./datasets/Collection5/28_11_12g.txt  \n",
            "  inflating: ./datasets/Collection5/28_11_12h.ann  \n",
            "  inflating: ./datasets/Collection5/28_11_12h.txt  \n",
            "  inflating: ./datasets/Collection5/28_11_12i.ann  \n",
            "  inflating: ./datasets/Collection5/28_11_12i.txt  \n",
            "  inflating: ./datasets/Collection5/28_11_12j.ann  \n",
            "  inflating: ./datasets/Collection5/28_11_12j.txt  \n",
            "  inflating: ./datasets/Collection5/290.ann  \n",
            "  inflating: ./datasets/Collection5/290.txt  \n",
            "  inflating: ./datasets/Collection5/291.ann  \n",
            "  inflating: ./datasets/Collection5/291.txt  \n",
            "  inflating: ./datasets/Collection5/292.ann  \n",
            "  inflating: ./datasets/Collection5/292.txt  \n",
            "  inflating: ./datasets/Collection5/293.ann  \n",
            "  inflating: ./datasets/Collection5/293.txt  \n",
            "  inflating: ./datasets/Collection5/294.ann  \n",
            "  inflating: ./datasets/Collection5/294.txt  \n",
            "  inflating: ./datasets/Collection5/295.ann  \n",
            "  inflating: ./datasets/Collection5/295.txt  \n",
            "  inflating: ./datasets/Collection5/296.ann  \n",
            "  inflating: ./datasets/Collection5/296.txt  \n",
            "  inflating: ./datasets/Collection5/297.ann  \n",
            "  inflating: ./datasets/Collection5/297.txt  \n",
            "  inflating: ./datasets/Collection5/298.ann  \n",
            "  inflating: ./datasets/Collection5/298.txt  \n",
            "  inflating: ./datasets/Collection5/299.ann  \n",
            "  inflating: ./datasets/Collection5/299.txt  \n",
            "  inflating: ./datasets/Collection5/29_11_12a.ann  \n",
            "  inflating: ./datasets/Collection5/29_11_12a.txt  \n",
            "  inflating: ./datasets/Collection5/29_11_12b.ann  \n",
            "  inflating: ./datasets/Collection5/29_11_12b.txt  \n",
            "  inflating: ./datasets/Collection5/300.ann  \n",
            "  inflating: ./datasets/Collection5/300.txt  \n",
            "  inflating: ./datasets/Collection5/301.ann  \n",
            "  inflating: ./datasets/Collection5/301.txt  \n",
            "  inflating: ./datasets/Collection5/302.ann  \n",
            "  inflating: ./datasets/Collection5/302.txt  \n",
            "  inflating: ./datasets/Collection5/303.ann  \n",
            "  inflating: ./datasets/Collection5/303.txt  \n",
            "  inflating: ./datasets/Collection5/304.ann  \n",
            "  inflating: ./datasets/Collection5/304.txt  \n",
            "  inflating: ./datasets/Collection5/305.ann  \n",
            "  inflating: ./datasets/Collection5/305.txt  \n",
            "  inflating: ./datasets/Collection5/306.ann  \n",
            "  inflating: ./datasets/Collection5/306.txt  \n",
            "  inflating: ./datasets/Collection5/307.ann  \n",
            "  inflating: ./datasets/Collection5/307.txt  \n",
            "  inflating: ./datasets/Collection5/308.ann  \n",
            "  inflating: ./datasets/Collection5/308.txt  \n",
            "  inflating: ./datasets/Collection5/309.ann  \n",
            "  inflating: ./datasets/Collection5/309.txt  \n",
            "  inflating: ./datasets/Collection5/30_11_12b.ann  \n",
            "  inflating: ./datasets/Collection5/30_11_12b.txt  \n",
            "  inflating: ./datasets/Collection5/30_11_12h.ann  \n",
            "  inflating: ./datasets/Collection5/30_11_12h.txt  \n",
            "  inflating: ./datasets/Collection5/30_11_12i.ann  \n",
            "  inflating: ./datasets/Collection5/30_11_12i.txt  \n",
            "  inflating: ./datasets/Collection5/310.ann  \n",
            "  inflating: ./datasets/Collection5/310.txt  \n",
            "  inflating: ./datasets/Collection5/311.ann  \n",
            "  inflating: ./datasets/Collection5/311.txt  \n",
            "  inflating: ./datasets/Collection5/312.ann  \n",
            "  inflating: ./datasets/Collection5/312.txt  \n",
            "  inflating: ./datasets/Collection5/313.ann  \n",
            "  inflating: ./datasets/Collection5/313.txt  \n",
            "  inflating: ./datasets/Collection5/314.ann  \n",
            "  inflating: ./datasets/Collection5/314.txt  \n",
            "  inflating: ./datasets/Collection5/315.ann  \n",
            "  inflating: ./datasets/Collection5/315.txt  \n",
            "  inflating: ./datasets/Collection5/316.ann  \n",
            "  inflating: ./datasets/Collection5/316.txt  \n",
            "  inflating: ./datasets/Collection5/317.ann  \n",
            "  inflating: ./datasets/Collection5/317.txt  \n",
            "  inflating: ./datasets/Collection5/318.ann  \n",
            "  inflating: ./datasets/Collection5/318.txt  \n",
            "  inflating: ./datasets/Collection5/319.ann  \n",
            "  inflating: ./datasets/Collection5/319.txt  \n",
            "  inflating: ./datasets/Collection5/320.ann  \n",
            "  inflating: ./datasets/Collection5/320.txt  \n",
            "  inflating: ./datasets/Collection5/321.ann  \n",
            "  inflating: ./datasets/Collection5/321.txt  \n",
            "  inflating: ./datasets/Collection5/322.ann  \n",
            "  inflating: ./datasets/Collection5/322.txt  \n",
            "  inflating: ./datasets/Collection5/323.ann  \n",
            "  inflating: ./datasets/Collection5/323.txt  \n",
            "  inflating: ./datasets/Collection5/324.ann  \n",
            "  inflating: ./datasets/Collection5/324.txt  \n",
            "  inflating: ./datasets/Collection5/325.ann  \n",
            "  inflating: ./datasets/Collection5/325.txt  \n",
            "  inflating: ./datasets/Collection5/326.ann  \n",
            "  inflating: ./datasets/Collection5/326.txt  \n",
            "  inflating: ./datasets/Collection5/327.ann  \n",
            "  inflating: ./datasets/Collection5/327.txt  \n",
            "  inflating: ./datasets/Collection5/328.ann  \n",
            "  inflating: ./datasets/Collection5/328.txt  \n",
            "  inflating: ./datasets/Collection5/329.ann  \n",
            "  inflating: ./datasets/Collection5/329.txt  \n",
            "  inflating: ./datasets/Collection5/330.ann  \n",
            "  inflating: ./datasets/Collection5/330.txt  \n",
            "  inflating: ./datasets/Collection5/331.ann  \n",
            "  inflating: ./datasets/Collection5/331.txt  \n",
            "  inflating: ./datasets/Collection5/332.ann  \n",
            "  inflating: ./datasets/Collection5/332.txt  \n",
            "  inflating: ./datasets/Collection5/333.ann  \n",
            "  inflating: ./datasets/Collection5/333.txt  \n",
            "  inflating: ./datasets/Collection5/334.ann  \n",
            "  inflating: ./datasets/Collection5/334.txt  \n",
            "  inflating: ./datasets/Collection5/335.ann  \n",
            "  inflating: ./datasets/Collection5/335.txt  \n",
            "  inflating: ./datasets/Collection5/336.ann  \n",
            "  inflating: ./datasets/Collection5/336.txt  \n",
            "  inflating: ./datasets/Collection5/337.ann  \n",
            "  inflating: ./datasets/Collection5/337.txt  \n",
            "  inflating: ./datasets/Collection5/338.ann  \n",
            "  inflating: ./datasets/Collection5/338.txt  \n",
            "  inflating: ./datasets/Collection5/339.ann  \n",
            "  inflating: ./datasets/Collection5/339.txt  \n",
            "  inflating: ./datasets/Collection5/340.ann  \n",
            "  inflating: ./datasets/Collection5/340.txt  \n",
            "  inflating: ./datasets/Collection5/341.ann  \n",
            "  inflating: ./datasets/Collection5/341.txt  \n",
            "  inflating: ./datasets/Collection5/342.ann  \n",
            "  inflating: ./datasets/Collection5/342.txt  \n",
            "  inflating: ./datasets/Collection5/343.ann  \n",
            "  inflating: ./datasets/Collection5/343.txt  \n",
            "  inflating: ./datasets/Collection5/344.ann  \n",
            "  inflating: ./datasets/Collection5/344.txt  \n",
            "  inflating: ./datasets/Collection5/345.ann  \n",
            "  inflating: ./datasets/Collection5/345.txt  \n",
            "  inflating: ./datasets/Collection5/346.ann  \n",
            "  inflating: ./datasets/Collection5/346.txt  \n",
            "  inflating: ./datasets/Collection5/347.ann  \n",
            "  inflating: ./datasets/Collection5/347.txt  \n",
            "  inflating: ./datasets/Collection5/348.ann  \n",
            "  inflating: ./datasets/Collection5/348.txt  \n",
            "  inflating: ./datasets/Collection5/349.ann  \n",
            "  inflating: ./datasets/Collection5/349.txt  \n",
            "  inflating: ./datasets/Collection5/350.ann  \n",
            "  inflating: ./datasets/Collection5/350.txt  \n",
            "  inflating: ./datasets/Collection5/351.ann  \n",
            "  inflating: ./datasets/Collection5/351.txt  \n",
            "  inflating: ./datasets/Collection5/352.ann  \n",
            "  inflating: ./datasets/Collection5/352.txt  \n",
            "  inflating: ./datasets/Collection5/353.ann  \n",
            "  inflating: ./datasets/Collection5/353.txt  \n",
            "  inflating: ./datasets/Collection5/354.ann  \n",
            "  inflating: ./datasets/Collection5/354.txt  \n",
            "  inflating: ./datasets/Collection5/355.ann  \n",
            "  inflating: ./datasets/Collection5/355.txt  \n",
            "  inflating: ./datasets/Collection5/356.ann  \n",
            "  inflating: ./datasets/Collection5/356.txt  \n",
            "  inflating: ./datasets/Collection5/357.ann  \n",
            "  inflating: ./datasets/Collection5/357.txt  \n",
            "  inflating: ./datasets/Collection5/358.ann  \n",
            "  inflating: ./datasets/Collection5/358.txt  \n",
            "  inflating: ./datasets/Collection5/359.ann  \n",
            "  inflating: ./datasets/Collection5/359.txt  \n",
            "  inflating: ./datasets/Collection5/360.ann  \n",
            "  inflating: ./datasets/Collection5/360.txt  \n",
            "  inflating: ./datasets/Collection5/361.ann  \n",
            "  inflating: ./datasets/Collection5/361.txt  \n",
            "  inflating: ./datasets/Collection5/362.ann  \n",
            "  inflating: ./datasets/Collection5/362.txt  \n",
            "  inflating: ./datasets/Collection5/363.ann  \n",
            "  inflating: ./datasets/Collection5/363.txt  \n",
            "  inflating: ./datasets/Collection5/364.ann  \n",
            "  inflating: ./datasets/Collection5/364.txt  \n",
            "  inflating: ./datasets/Collection5/365.ann  \n",
            "  inflating: ./datasets/Collection5/365.txt  \n",
            "  inflating: ./datasets/Collection5/366.ann  \n",
            "  inflating: ./datasets/Collection5/366.txt  \n",
            "  inflating: ./datasets/Collection5/367.ann  \n",
            "  inflating: ./datasets/Collection5/367.txt  \n",
            "  inflating: ./datasets/Collection5/368.ann  \n",
            "  inflating: ./datasets/Collection5/368.txt  \n",
            "  inflating: ./datasets/Collection5/369.ann  \n",
            "  inflating: ./datasets/Collection5/369.txt  \n",
            "  inflating: ./datasets/Collection5/370.ann  \n",
            "  inflating: ./datasets/Collection5/370.txt  \n",
            "  inflating: ./datasets/Collection5/371.ann  \n",
            "  inflating: ./datasets/Collection5/371.txt  \n",
            "  inflating: ./datasets/Collection5/372.ann  \n",
            "  inflating: ./datasets/Collection5/372.txt  \n",
            "  inflating: ./datasets/Collection5/373.ann  \n",
            "  inflating: ./datasets/Collection5/373.txt  \n",
            "  inflating: ./datasets/Collection5/374.ann  \n",
            "  inflating: ./datasets/Collection5/374.txt  \n",
            "  inflating: ./datasets/Collection5/375.ann  \n",
            "  inflating: ./datasets/Collection5/375.txt  \n",
            "  inflating: ./datasets/Collection5/376.ann  \n",
            "  inflating: ./datasets/Collection5/376.txt  \n",
            "  inflating: ./datasets/Collection5/377.ann  \n",
            "  inflating: ./datasets/Collection5/377.txt  \n",
            "  inflating: ./datasets/Collection5/378.ann  \n",
            "  inflating: ./datasets/Collection5/378.txt  \n",
            "  inflating: ./datasets/Collection5/379.ann  \n",
            "  inflating: ./datasets/Collection5/379.txt  \n",
            "  inflating: ./datasets/Collection5/380.ann  \n",
            "  inflating: ./datasets/Collection5/380.txt  \n",
            "  inflating: ./datasets/Collection5/381.ann  \n",
            "  inflating: ./datasets/Collection5/381.txt  \n",
            "  inflating: ./datasets/Collection5/382.ann  \n",
            "  inflating: ./datasets/Collection5/382.txt  \n",
            "  inflating: ./datasets/Collection5/383.ann  \n",
            "  inflating: ./datasets/Collection5/383.txt  \n",
            "  inflating: ./datasets/Collection5/384.ann  \n",
            "  inflating: ./datasets/Collection5/384.txt  \n",
            "  inflating: ./datasets/Collection5/385.ann  \n",
            "  inflating: ./datasets/Collection5/385.txt  \n",
            "  inflating: ./datasets/Collection5/386.ann  \n",
            "  inflating: ./datasets/Collection5/386.txt  \n",
            "  inflating: ./datasets/Collection5/387.ann  \n",
            "  inflating: ./datasets/Collection5/387.txt  \n",
            "  inflating: ./datasets/Collection5/388.ann  \n",
            "  inflating: ./datasets/Collection5/388.txt  \n",
            "  inflating: ./datasets/Collection5/389.ann  \n",
            "  inflating: ./datasets/Collection5/389.txt  \n",
            "  inflating: ./datasets/Collection5/390.ann  \n",
            "  inflating: ./datasets/Collection5/390.txt  \n",
            "  inflating: ./datasets/Collection5/391.ann  \n",
            "  inflating: ./datasets/Collection5/391.txt  \n",
            "  inflating: ./datasets/Collection5/392.ann  \n",
            "  inflating: ./datasets/Collection5/392.txt  \n",
            "  inflating: ./datasets/Collection5/393.ann  \n",
            "  inflating: ./datasets/Collection5/393.txt  \n",
            "  inflating: ./datasets/Collection5/394.ann  \n",
            "  inflating: ./datasets/Collection5/394.txt  \n",
            "  inflating: ./datasets/Collection5/395.ann  \n",
            "  inflating: ./datasets/Collection5/395.txt  \n",
            "  inflating: ./datasets/Collection5/396.ann  \n",
            "  inflating: ./datasets/Collection5/396.txt  \n",
            "  inflating: ./datasets/Collection5/397.ann  \n",
            "  inflating: ./datasets/Collection5/397.txt  \n",
            "  inflating: ./datasets/Collection5/398.ann  \n",
            "  inflating: ./datasets/Collection5/398.txt  \n",
            "  inflating: ./datasets/Collection5/399.ann  \n",
            "  inflating: ./datasets/Collection5/399.txt  \n",
            "  inflating: ./datasets/Collection5/400.ann  \n",
            "  inflating: ./datasets/Collection5/400.txt  \n",
            "  inflating: ./datasets/Collection5/401.ann  \n",
            "  inflating: ./datasets/Collection5/401.txt  \n",
            "  inflating: ./datasets/Collection5/402.ann  \n",
            "  inflating: ./datasets/Collection5/402.txt  \n",
            "  inflating: ./datasets/Collection5/403.ann  \n",
            "  inflating: ./datasets/Collection5/403.txt  \n",
            "  inflating: ./datasets/Collection5/404.ann  \n",
            "  inflating: ./datasets/Collection5/404.txt  \n",
            "  inflating: ./datasets/Collection5/405.ann  \n",
            "  inflating: ./datasets/Collection5/405.txt  \n",
            "  inflating: ./datasets/Collection5/406.ann  \n",
            "  inflating: ./datasets/Collection5/406.txt  \n",
            "  inflating: ./datasets/Collection5/407.ann  \n",
            "  inflating: ./datasets/Collection5/407.txt  \n",
            "  inflating: ./datasets/Collection5/408.ann  \n",
            "  inflating: ./datasets/Collection5/408.txt  \n",
            "  inflating: ./datasets/Collection5/409.ann  \n",
            "  inflating: ./datasets/Collection5/409.txt  \n",
            "  inflating: ./datasets/Collection5/410.ann  \n",
            "  inflating: ./datasets/Collection5/410.txt  \n",
            "  inflating: ./datasets/Collection5/411.ann  \n",
            "  inflating: ./datasets/Collection5/411.txt  \n",
            "  inflating: ./datasets/Collection5/412.ann  \n",
            "  inflating: ./datasets/Collection5/412.txt  \n",
            "  inflating: ./datasets/Collection5/413.ann  \n",
            "  inflating: ./datasets/Collection5/413.txt  \n",
            "  inflating: ./datasets/Collection5/414.ann  \n",
            "  inflating: ./datasets/Collection5/414.txt  \n",
            "  inflating: ./datasets/Collection5/415.ann  \n",
            "  inflating: ./datasets/Collection5/415.txt  \n",
            "  inflating: ./datasets/Collection5/416.ann  \n",
            "  inflating: ./datasets/Collection5/416.txt  \n",
            "  inflating: ./datasets/Collection5/417.ann  \n",
            "  inflating: ./datasets/Collection5/417.txt  \n",
            "  inflating: ./datasets/Collection5/418.ann  \n",
            "  inflating: ./datasets/Collection5/418.txt  \n",
            "  inflating: ./datasets/Collection5/419.ann  \n",
            "  inflating: ./datasets/Collection5/419.txt  \n",
            "  inflating: ./datasets/Collection5/420.ann  \n",
            "  inflating: ./datasets/Collection5/420.txt  \n",
            "  inflating: ./datasets/Collection5/421.ann  \n",
            "  inflating: ./datasets/Collection5/421.txt  \n",
            "  inflating: ./datasets/Collection5/422.ann  \n",
            "  inflating: ./datasets/Collection5/422.txt  \n",
            "  inflating: ./datasets/Collection5/423.ann  \n",
            "  inflating: ./datasets/Collection5/423.txt  \n",
            "  inflating: ./datasets/Collection5/424.ann  \n",
            "  inflating: ./datasets/Collection5/424.txt  \n",
            "  inflating: ./datasets/Collection5/425.ann  \n",
            "  inflating: ./datasets/Collection5/425.txt  \n",
            "  inflating: ./datasets/Collection5/426.ann  \n",
            "  inflating: ./datasets/Collection5/426.txt  \n",
            "  inflating: ./datasets/Collection5/427.ann  \n",
            "  inflating: ./datasets/Collection5/427.txt  \n",
            "  inflating: ./datasets/Collection5/428.ann  \n",
            "  inflating: ./datasets/Collection5/428.txt  \n",
            "  inflating: ./datasets/Collection5/429.ann  \n",
            "  inflating: ./datasets/Collection5/429.txt  \n",
            "  inflating: ./datasets/Collection5/430.ann  \n",
            "  inflating: ./datasets/Collection5/430.txt  \n",
            "  inflating: ./datasets/Collection5/431.ann  \n",
            "  inflating: ./datasets/Collection5/431.txt  \n",
            "  inflating: ./datasets/Collection5/432.ann  \n",
            "  inflating: ./datasets/Collection5/432.txt  \n",
            "  inflating: ./datasets/Collection5/433.ann  \n",
            "  inflating: ./datasets/Collection5/433.txt  \n",
            "  inflating: ./datasets/Collection5/434.ann  \n",
            "  inflating: ./datasets/Collection5/434.txt  \n",
            "  inflating: ./datasets/Collection5/435.ann  \n",
            "  inflating: ./datasets/Collection5/435.txt  \n",
            "  inflating: ./datasets/Collection5/436.ann  \n",
            "  inflating: ./datasets/Collection5/436.txt  \n",
            "  inflating: ./datasets/Collection5/437.ann  \n",
            "  inflating: ./datasets/Collection5/437.txt  \n",
            "  inflating: ./datasets/Collection5/438.ann  \n",
            "  inflating: ./datasets/Collection5/438.txt  \n",
            "  inflating: ./datasets/Collection5/439.ann  \n",
            "  inflating: ./datasets/Collection5/439.txt  \n",
            "  inflating: ./datasets/Collection5/440.ann  \n",
            "  inflating: ./datasets/Collection5/440.txt  \n",
            "  inflating: ./datasets/Collection5/441.ann  \n",
            "  inflating: ./datasets/Collection5/441.txt  \n",
            "  inflating: ./datasets/Collection5/442.ann  \n",
            "  inflating: ./datasets/Collection5/442.txt  \n",
            "  inflating: ./datasets/Collection5/443.ann  \n",
            "  inflating: ./datasets/Collection5/443.txt  \n",
            "  inflating: ./datasets/Collection5/444.ann  \n",
            "  inflating: ./datasets/Collection5/444.txt  \n",
            "  inflating: ./datasets/Collection5/445.ann  \n",
            "  inflating: ./datasets/Collection5/445.txt  \n",
            "  inflating: ./datasets/Collection5/446.ann  \n",
            "  inflating: ./datasets/Collection5/446.txt  \n",
            "  inflating: ./datasets/Collection5/447.ann  \n",
            "  inflating: ./datasets/Collection5/447.txt  \n",
            "  inflating: ./datasets/Collection5/448.ann  \n",
            "  inflating: ./datasets/Collection5/448.txt  \n",
            "  inflating: ./datasets/Collection5/449.ann  \n",
            "  inflating: ./datasets/Collection5/449.txt  \n",
            "  inflating: ./datasets/Collection5/450.ann  \n",
            "  inflating: ./datasets/Collection5/450.txt  \n",
            "  inflating: ./datasets/Collection5/451.ann  \n",
            "  inflating: ./datasets/Collection5/451.txt  \n",
            "  inflating: ./datasets/Collection5/452.ann  \n",
            "  inflating: ./datasets/Collection5/452.txt  \n",
            "  inflating: ./datasets/Collection5/453.ann  \n",
            "  inflating: ./datasets/Collection5/453.txt  \n",
            "  inflating: ./datasets/Collection5/454.ann  \n",
            "  inflating: ./datasets/Collection5/454.txt  \n",
            "  inflating: ./datasets/Collection5/455.ann  \n",
            "  inflating: ./datasets/Collection5/455.txt  \n",
            "  inflating: ./datasets/Collection5/457.ann  \n",
            "  inflating: ./datasets/Collection5/457.txt  \n",
            "  inflating: ./datasets/Collection5/458.ann  \n",
            "  inflating: ./datasets/Collection5/458.txt  \n",
            "  inflating: ./datasets/Collection5/459.ann  \n",
            "  inflating: ./datasets/Collection5/459.txt  \n",
            "  inflating: ./datasets/Collection5/460.ann  \n",
            "  inflating: ./datasets/Collection5/460.txt  \n",
            "  inflating: ./datasets/Collection5/461.ann  \n",
            "  inflating: ./datasets/Collection5/461.txt  \n",
            "  inflating: ./datasets/Collection5/462.ann  \n",
            "  inflating: ./datasets/Collection5/462.txt  \n",
            "  inflating: ./datasets/Collection5/463.ann  \n",
            "  inflating: ./datasets/Collection5/463.txt  \n",
            "  inflating: ./datasets/Collection5/464.ann  \n",
            "  inflating: ./datasets/Collection5/464.txt  \n",
            "  inflating: ./datasets/Collection5/465.ann  \n",
            "  inflating: ./datasets/Collection5/465.txt  \n",
            "  inflating: ./datasets/Collection5/466.ann  \n",
            "  inflating: ./datasets/Collection5/466.txt  \n",
            "  inflating: ./datasets/Collection5/467.ann  \n",
            "  inflating: ./datasets/Collection5/467.txt  \n",
            "  inflating: ./datasets/Collection5/468.ann  \n",
            "  inflating: ./datasets/Collection5/468.txt  \n",
            "  inflating: ./datasets/Collection5/469.ann  \n",
            "  inflating: ./datasets/Collection5/469.txt  \n",
            "  inflating: ./datasets/Collection5/470.ann  \n",
            "  inflating: ./datasets/Collection5/470.txt  \n",
            "  inflating: ./datasets/Collection5/471.ann  \n",
            "  inflating: ./datasets/Collection5/471.txt  \n",
            "  inflating: ./datasets/Collection5/472.ann  \n",
            "  inflating: ./datasets/Collection5/472.txt  \n",
            "  inflating: ./datasets/Collection5/473.ann  \n",
            "  inflating: ./datasets/Collection5/473.txt  \n",
            "  inflating: ./datasets/Collection5/474.ann  \n",
            "  inflating: ./datasets/Collection5/474.txt  \n",
            "  inflating: ./datasets/Collection5/475.ann  \n",
            "  inflating: ./datasets/Collection5/475.txt  \n",
            "  inflating: ./datasets/Collection5/476.ann  \n",
            "  inflating: ./datasets/Collection5/476.txt  \n",
            "  inflating: ./datasets/Collection5/477.ann  \n",
            "  inflating: ./datasets/Collection5/477.txt  \n",
            "  inflating: ./datasets/Collection5/478.ann  \n",
            "  inflating: ./datasets/Collection5/478.txt  \n",
            "  inflating: ./datasets/Collection5/479.ann  \n",
            "  inflating: ./datasets/Collection5/479.txt  \n",
            "  inflating: ./datasets/Collection5/480.ann  \n",
            "  inflating: ./datasets/Collection5/480.txt  \n",
            "  inflating: ./datasets/Collection5/481.ann  \n",
            "  inflating: ./datasets/Collection5/481.txt  \n",
            "  inflating: ./datasets/Collection5/482.ann  \n",
            "  inflating: ./datasets/Collection5/482.txt  \n",
            "  inflating: ./datasets/Collection5/483.ann  \n",
            "  inflating: ./datasets/Collection5/483.txt  \n",
            "  inflating: ./datasets/Collection5/484.ann  \n",
            "  inflating: ./datasets/Collection5/484.txt  \n",
            "  inflating: ./datasets/Collection5/485.ann  \n",
            "  inflating: ./datasets/Collection5/485.txt  \n",
            "  inflating: ./datasets/Collection5/486.ann  \n",
            "  inflating: ./datasets/Collection5/486.txt  \n",
            "  inflating: ./datasets/Collection5/487.ann  \n",
            "  inflating: ./datasets/Collection5/487.txt  \n",
            "  inflating: ./datasets/Collection5/488.ann  \n",
            "  inflating: ./datasets/Collection5/488.txt  \n",
            "  inflating: ./datasets/Collection5/489.ann  \n",
            "  inflating: ./datasets/Collection5/489.txt  \n",
            "  inflating: ./datasets/Collection5/490.ann  \n",
            "  inflating: ./datasets/Collection5/490.txt  \n",
            "  inflating: ./datasets/Collection5/491.ann  \n",
            "  inflating: ./datasets/Collection5/491.txt  \n",
            "  inflating: ./datasets/Collection5/492.ann  \n",
            "  inflating: ./datasets/Collection5/492.txt  \n",
            "  inflating: ./datasets/Collection5/493.ann  \n",
            "  inflating: ./datasets/Collection5/493.txt  \n",
            "  inflating: ./datasets/Collection5/494.ann  \n",
            "  inflating: ./datasets/Collection5/494.txt  \n",
            "  inflating: ./datasets/Collection5/495.ann  \n",
            "  inflating: ./datasets/Collection5/495.txt  \n",
            "  inflating: ./datasets/Collection5/496.ann  \n",
            "  inflating: ./datasets/Collection5/496.txt  \n",
            "  inflating: ./datasets/Collection5/497.ann  \n",
            "  inflating: ./datasets/Collection5/497.txt  \n",
            "  inflating: ./datasets/Collection5/498.ann  \n",
            "  inflating: ./datasets/Collection5/498.txt  \n",
            "  inflating: ./datasets/Collection5/499.ann  \n",
            "  inflating: ./datasets/Collection5/499.txt  \n",
            "  inflating: ./datasets/Collection5/500.ann  \n",
            "  inflating: ./datasets/Collection5/500.txt  \n",
            "  inflating: ./datasets/Collection5/501.ann  \n",
            "  inflating: ./datasets/Collection5/501.txt  \n",
            "  inflating: ./datasets/Collection5/502.ann  \n",
            "  inflating: ./datasets/Collection5/502.txt  \n",
            "  inflating: ./datasets/Collection5/503.ann  \n",
            "  inflating: ./datasets/Collection5/503.txt  \n",
            "  inflating: ./datasets/Collection5/504.ann  \n",
            "  inflating: ./datasets/Collection5/504.txt  \n",
            "  inflating: ./datasets/Collection5/505.ann  \n",
            "  inflating: ./datasets/Collection5/505.txt  \n",
            "  inflating: ./datasets/Collection5/506.ann  \n",
            "  inflating: ./datasets/Collection5/506.txt  \n",
            "  inflating: ./datasets/Collection5/507.ann  \n",
            "  inflating: ./datasets/Collection5/507.txt  \n",
            "  inflating: ./datasets/Collection5/508.ann  \n",
            "  inflating: ./datasets/Collection5/508.txt  \n",
            "  inflating: ./datasets/Collection5/509.ann  \n",
            "  inflating: ./datasets/Collection5/509.txt  \n",
            "  inflating: ./datasets/Collection5/510.ann  \n",
            "  inflating: ./datasets/Collection5/510.txt  \n",
            "  inflating: ./datasets/Collection5/511.ann  \n",
            "  inflating: ./datasets/Collection5/511.txt  \n",
            "  inflating: ./datasets/Collection5/512.ann  \n",
            "  inflating: ./datasets/Collection5/512.txt  \n",
            "  inflating: ./datasets/Collection5/513.ann  \n",
            "  inflating: ./datasets/Collection5/513.txt  \n",
            "  inflating: ./datasets/Collection5/514.ann  \n",
            "  inflating: ./datasets/Collection5/514.txt  \n",
            "  inflating: ./datasets/Collection5/515.ann  \n",
            "  inflating: ./datasets/Collection5/515.txt  \n",
            "  inflating: ./datasets/Collection5/516.ann  \n",
            "  inflating: ./datasets/Collection5/516.txt  \n",
            "  inflating: ./datasets/Collection5/517.ann  \n",
            "  inflating: ./datasets/Collection5/517.txt  \n",
            "  inflating: ./datasets/Collection5/518.ann  \n",
            "  inflating: ./datasets/Collection5/518.txt  \n",
            "  inflating: ./datasets/Collection5/519.ann  \n",
            "  inflating: ./datasets/Collection5/519.txt  \n",
            "  inflating: ./datasets/Collection5/520.ann  \n",
            "  inflating: ./datasets/Collection5/520.txt  \n",
            "  inflating: ./datasets/Collection5/521.ann  \n",
            "  inflating: ./datasets/Collection5/521.txt  \n",
            "  inflating: ./datasets/Collection5/522.ann  \n",
            "  inflating: ./datasets/Collection5/522.txt  \n",
            "  inflating: ./datasets/Collection5/523.ann  \n",
            "  inflating: ./datasets/Collection5/523.txt  \n",
            "  inflating: ./datasets/Collection5/524.ann  \n",
            "  inflating: ./datasets/Collection5/524.txt  \n",
            "  inflating: ./datasets/Collection5/525.ann  \n",
            "  inflating: ./datasets/Collection5/525.txt  \n",
            "  inflating: ./datasets/Collection5/526.ann  \n",
            "  inflating: ./datasets/Collection5/526.txt  \n",
            "  inflating: ./datasets/Collection5/527.ann  \n",
            "  inflating: ./datasets/Collection5/527.txt  \n",
            "  inflating: ./datasets/Collection5/528.ann  \n",
            "  inflating: ./datasets/Collection5/528.txt  \n",
            "  inflating: ./datasets/Collection5/529.ann  \n",
            "  inflating: ./datasets/Collection5/529.txt  \n",
            "  inflating: ./datasets/Collection5/530.ann  \n",
            "  inflating: ./datasets/Collection5/530.txt  \n",
            "  inflating: ./datasets/Collection5/531.ann  \n",
            "  inflating: ./datasets/Collection5/531.txt  \n",
            "  inflating: ./datasets/Collection5/532.ann  \n",
            "  inflating: ./datasets/Collection5/532.txt  \n",
            "  inflating: ./datasets/Collection5/533 (!).ann  \n",
            "  inflating: ./datasets/Collection5/533 (!).txt  \n",
            "  inflating: ./datasets/Collection5/534.ann  \n",
            "  inflating: ./datasets/Collection5/534.txt  \n",
            "  inflating: ./datasets/Collection5/535.ann  \n",
            "  inflating: ./datasets/Collection5/535.txt  \n",
            "  inflating: ./datasets/Collection5/536.ann  \n",
            "  inflating: ./datasets/Collection5/536.txt  \n",
            "  inflating: ./datasets/Collection5/537.ann  \n",
            "  inflating: ./datasets/Collection5/537.txt  \n",
            "  inflating: ./datasets/Collection5/538.ann  \n",
            "  inflating: ./datasets/Collection5/538.txt  \n",
            "  inflating: ./datasets/Collection5/539.ann  \n",
            "  inflating: ./datasets/Collection5/539.txt  \n",
            "  inflating: ./datasets/Collection5/540.ann  \n",
            "  inflating: ./datasets/Collection5/540.txt  \n",
            "  inflating: ./datasets/Collection5/541.ann  \n",
            "  inflating: ./datasets/Collection5/541.txt  \n",
            "  inflating: ./datasets/Collection5/542.ann  \n",
            "  inflating: ./datasets/Collection5/542.txt  \n",
            "  inflating: ./datasets/Collection5/543.ann  \n",
            "  inflating: ./datasets/Collection5/543.txt  \n",
            "  inflating: ./datasets/Collection5/544.ann  \n",
            "  inflating: ./datasets/Collection5/544.txt  \n",
            "  inflating: ./datasets/Collection5/545.ann  \n",
            "  inflating: ./datasets/Collection5/545.txt  \n",
            "  inflating: ./datasets/Collection5/546.ann  \n",
            "  inflating: ./datasets/Collection5/546.txt  \n",
            "  inflating: ./datasets/Collection5/547.ann  \n",
            "  inflating: ./datasets/Collection5/547.txt  \n",
            "  inflating: ./datasets/Collection5/548.ann  \n",
            "  inflating: ./datasets/Collection5/548.txt  \n",
            "  inflating: ./datasets/Collection5/549.ann  \n",
            "  inflating: ./datasets/Collection5/549.txt  \n",
            "  inflating: ./datasets/Collection5/550.ann  \n",
            "  inflating: ./datasets/Collection5/550.txt  \n",
            "  inflating: ./datasets/Collection5/551.ann  \n",
            "  inflating: ./datasets/Collection5/551.txt  \n",
            "  inflating: ./datasets/Collection5/552.ann  \n",
            "  inflating: ./datasets/Collection5/552.txt  \n",
            "  inflating: ./datasets/Collection5/553.ann  \n",
            "  inflating: ./datasets/Collection5/553.txt  \n",
            "  inflating: ./datasets/Collection5/554.ann  \n",
            "  inflating: ./datasets/Collection5/554.txt  \n",
            "  inflating: ./datasets/Collection5/555 (!).ann  \n",
            "  inflating: ./datasets/Collection5/555 (!).txt  \n",
            "  inflating: ./datasets/Collection5/556.ann  \n",
            "  inflating: ./datasets/Collection5/556.txt  \n",
            "  inflating: ./datasets/Collection5/557.ann  \n",
            "  inflating: ./datasets/Collection5/557.txt  \n",
            "  inflating: ./datasets/Collection5/558.ann  \n",
            "  inflating: ./datasets/Collection5/558.txt  \n",
            "  inflating: ./datasets/Collection5/559.ann  \n",
            "  inflating: ./datasets/Collection5/559.txt  \n",
            "  inflating: ./datasets/Collection5/560.ann  \n",
            "  inflating: ./datasets/Collection5/560.txt  \n",
            "  inflating: ./datasets/Collection5/561.ann  \n",
            "  inflating: ./datasets/Collection5/561.txt  \n",
            "  inflating: ./datasets/Collection5/562.ann  \n",
            "  inflating: ./datasets/Collection5/562.txt  \n",
            "  inflating: ./datasets/Collection5/563.ann  \n",
            "  inflating: ./datasets/Collection5/563.txt  \n",
            "  inflating: ./datasets/Collection5/564.ann  \n",
            "  inflating: ./datasets/Collection5/564.txt  \n",
            "  inflating: ./datasets/Collection5/565.ann  \n",
            "  inflating: ./datasets/Collection5/565.txt  \n",
            "  inflating: ./datasets/Collection5/567.ann  \n",
            "  inflating: ./datasets/Collection5/567.txt  \n",
            "  inflating: ./datasets/Collection5/568.ann  \n",
            "  inflating: ./datasets/Collection5/568.txt  \n",
            "  inflating: ./datasets/Collection5/569.ann  \n",
            "  inflating: ./datasets/Collection5/569.txt  \n",
            "  inflating: ./datasets/Collection5/570.ann  \n",
            "  inflating: ./datasets/Collection5/570.txt  \n",
            "  inflating: ./datasets/Collection5/571.ann  \n",
            "  inflating: ./datasets/Collection5/571.txt  \n",
            "  inflating: ./datasets/Collection5/572.ann  \n",
            "  inflating: ./datasets/Collection5/572.txt  \n",
            "  inflating: ./datasets/Collection5/574.ann  \n",
            "  inflating: ./datasets/Collection5/574.txt  \n",
            "  inflating: ./datasets/Collection5/575.ann  \n",
            "  inflating: ./datasets/Collection5/575.txt  \n",
            "  inflating: ./datasets/Collection5/576.ann  \n",
            "  inflating: ./datasets/Collection5/576.txt  \n",
            "  inflating: ./datasets/Collection5/577.ann  \n",
            "  inflating: ./datasets/Collection5/577.txt  \n",
            "  inflating: ./datasets/Collection5/578.ann  \n",
            "  inflating: ./datasets/Collection5/578.txt  \n",
            "  inflating: ./datasets/Collection5/579.ann  \n",
            "  inflating: ./datasets/Collection5/579.txt  \n",
            "  inflating: ./datasets/Collection5/581.ann  \n",
            "  inflating: ./datasets/Collection5/581.txt  \n",
            "  inflating: ./datasets/Collection5/582.ann  \n",
            "  inflating: ./datasets/Collection5/582.txt  \n",
            "  inflating: ./datasets/Collection5/583.ann  \n",
            "  inflating: ./datasets/Collection5/583.txt  \n",
            "  inflating: ./datasets/Collection5/584 (!).ann  \n",
            "  inflating: ./datasets/Collection5/584 (!).txt  \n",
            "  inflating: ./datasets/Collection5/585.ann  \n",
            "  inflating: ./datasets/Collection5/585.txt  \n",
            "  inflating: ./datasets/Collection5/586.ann  \n",
            "  inflating: ./datasets/Collection5/586.txt  \n",
            "  inflating: ./datasets/Collection5/587.ann  \n",
            "  inflating: ./datasets/Collection5/587.txt  \n",
            "  inflating: ./datasets/Collection5/588.ann  \n",
            "  inflating: ./datasets/Collection5/588.txt  \n",
            "  inflating: ./datasets/Collection5/589.ann  \n",
            "  inflating: ./datasets/Collection5/589.txt  \n",
            "  inflating: ./datasets/Collection5/590.ann  \n",
            "  inflating: ./datasets/Collection5/590.txt  \n",
            "  inflating: ./datasets/Collection5/591.ann  \n",
            "  inflating: ./datasets/Collection5/591.txt  \n",
            "  inflating: ./datasets/Collection5/592.ann  \n",
            "  inflating: ./datasets/Collection5/592.txt  \n",
            "  inflating: ./datasets/Collection5/593.ann  \n",
            "  inflating: ./datasets/Collection5/593.txt  \n",
            "  inflating: ./datasets/Collection5/594.ann  \n",
            "  inflating: ./datasets/Collection5/594.txt  \n",
            "  inflating: ./datasets/Collection5/595.ann  \n",
            "  inflating: ./datasets/Collection5/595.txt  \n",
            "  inflating: ./datasets/Collection5/596.ann  \n",
            "  inflating: ./datasets/Collection5/596.txt  \n",
            "  inflating: ./datasets/Collection5/597.ann  \n",
            "  inflating: ./datasets/Collection5/597.txt  \n",
            "  inflating: ./datasets/Collection5/598 (!).ann  \n",
            "  inflating: ./datasets/Collection5/598 (!).txt  \n",
            "  inflating: ./datasets/Collection5/599.ann  \n",
            "  inflating: ./datasets/Collection5/599.txt  \n",
            "  inflating: ./datasets/Collection5/600.ann  \n",
            "  inflating: ./datasets/Collection5/600.txt  \n",
            "  inflating: ./datasets/Collection5/601.ann  \n",
            "  inflating: ./datasets/Collection5/601.txt  \n",
            "  inflating: ./datasets/Collection5/602.ann  \n",
            "  inflating: ./datasets/Collection5/602.txt  \n",
            "  inflating: ./datasets/Collection5/610.ann  \n",
            "  inflating: ./datasets/Collection5/610.txt  \n",
            "  inflating: ./datasets/Collection5/611.ann  \n",
            "  inflating: ./datasets/Collection5/611.txt  \n",
            "  inflating: ./datasets/Collection5/612.ann  \n",
            "  inflating: ./datasets/Collection5/612.txt  \n",
            "  inflating: ./datasets/Collection5/613.ann  \n",
            "  inflating: ./datasets/Collection5/613.txt  \n",
            "  inflating: ./datasets/Collection5/614.ann  \n",
            "  inflating: ./datasets/Collection5/614.txt  \n",
            "  inflating: ./datasets/Collection5/615.ann  \n",
            "  inflating: ./datasets/Collection5/615.txt  \n",
            "  inflating: ./datasets/Collection5/616.ann  \n",
            "  inflating: ./datasets/Collection5/616.txt  \n",
            "  inflating: ./datasets/Collection5/617.ann  \n",
            "  inflating: ./datasets/Collection5/617.txt  \n",
            "  inflating: ./datasets/Collection5/618.ann  \n",
            "  inflating: ./datasets/Collection5/618.txt  \n",
            "  inflating: ./datasets/Collection5/619.ann  \n",
            "  inflating: ./datasets/Collection5/619.txt  \n",
            "  inflating: ./datasets/Collection5/620.ann  \n",
            "  inflating: ./datasets/Collection5/620.txt  \n",
            "  inflating: ./datasets/Collection5/621.ann  \n",
            "  inflating: ./datasets/Collection5/621.txt  \n",
            "  inflating: ./datasets/Collection5/622.ann  \n",
            "  inflating: ./datasets/Collection5/622.txt  \n",
            "  inflating: ./datasets/Collection5/623.ann  \n",
            "  inflating: ./datasets/Collection5/623.txt  \n",
            "  inflating: ./datasets/Collection5/624.ann  \n",
            "  inflating: ./datasets/Collection5/624.txt  \n",
            "  inflating: ./datasets/Collection5/625.ann  \n",
            "  inflating: ./datasets/Collection5/625.txt  \n",
            "  inflating: ./datasets/Collection5/626.ann  \n",
            "  inflating: ./datasets/Collection5/626.txt  \n",
            "  inflating: ./datasets/Collection5/627.ann  \n",
            "  inflating: ./datasets/Collection5/627.txt  \n",
            "  inflating: ./datasets/Collection5/628.ann  \n",
            "  inflating: ./datasets/Collection5/628.txt  \n",
            "  inflating: ./datasets/Collection5/629.ann  \n",
            "  inflating: ./datasets/Collection5/629.txt  \n",
            "  inflating: ./datasets/Collection5/630.ann  \n",
            "  inflating: ./datasets/Collection5/630.txt  \n",
            "  inflating: ./datasets/Collection5/631.ann  \n",
            "  inflating: ./datasets/Collection5/631.txt  \n",
            "  inflating: ./datasets/Collection5/632.ann  \n",
            "  inflating: ./datasets/Collection5/632.txt  \n",
            "  inflating: ./datasets/Collection5/633.ann  \n",
            "  inflating: ./datasets/Collection5/633.txt  \n",
            "  inflating: ./datasets/Collection5/abdulatipov.ann  \n",
            "  inflating: ./datasets/Collection5/abdulatipov.txt  \n",
            "  inflating: ./datasets/Collection5/artjakov.ann  \n",
            "  inflating: ./datasets/Collection5/artjakov.txt  \n",
            "  inflating: ./datasets/Collection5/Avtovaz.ann  \n",
            "  inflating: ./datasets/Collection5/Avtovaz.txt  \n",
            "  inflating: ./datasets/Collection5/blokhin.ann  \n",
            "  inflating: ./datasets/Collection5/blokhin.txt  \n",
            "  inflating: ./datasets/Collection5/chaves.ann  \n",
            "  inflating: ./datasets/Collection5/chaves.txt  \n",
            "  inflating: ./datasets/Collection5/chirkunov.ann  \n",
            "  inflating: ./datasets/Collection5/chirkunov.txt  \n",
            "  inflating: ./datasets/Collection5/kamchatka.ann  \n",
            "  inflating: ./datasets/Collection5/kamchatka.txt  \n",
            "  inflating: ./datasets/Collection5/klinton.ann  \n",
            "  inflating: ./datasets/Collection5/klinton.txt  \n",
            "  inflating: ./datasets/Collection5/kuleshov.ann  \n",
            "  inflating: ./datasets/Collection5/kuleshov.txt  \n",
            "  inflating: ./datasets/Collection5/last_01.ann  \n",
            "  inflating: ./datasets/Collection5/last_01.txt  \n",
            "  inflating: ./datasets/Collection5/last_02.ann  \n",
            "  inflating: ./datasets/Collection5/last_02.txt  \n",
            "  inflating: ./datasets/Collection5/last_03.ann  \n",
            "  inflating: ./datasets/Collection5/last_03.txt  \n",
            "  inflating: ./datasets/Collection5/last_04.ann  \n",
            "  inflating: ./datasets/Collection5/last_04.txt  \n",
            "  inflating: ./datasets/Collection5/last_05.ann  \n",
            "  inflating: ./datasets/Collection5/last_05.txt  \n",
            "  inflating: ./datasets/Collection5/last_06.ann  \n",
            "  inflating: ./datasets/Collection5/last_06.txt  \n",
            "  inflating: ./datasets/Collection5/last_07_new.ann  \n",
            "  inflating: ./datasets/Collection5/last_07_new.txt  \n",
            "  inflating: ./datasets/Collection5/last_08.ann  \n",
            "  inflating: ./datasets/Collection5/last_08.txt  \n",
            "  inflating: ./datasets/Collection5/last_09.ann  \n",
            "  inflating: ./datasets/Collection5/last_09.txt  \n",
            "  inflating: ./datasets/Collection5/last_10.ann  \n",
            "  inflating: ./datasets/Collection5/last_10.txt  \n",
            "  inflating: ./datasets/Collection5/last_11.ann  \n",
            "  inflating: ./datasets/Collection5/last_11.txt  \n",
            "  inflating: ./datasets/Collection5/last_12.ann  \n",
            "  inflating: ./datasets/Collection5/last_12.txt  \n",
            "  inflating: ./datasets/Collection5/last_13.ann  \n",
            "  inflating: ./datasets/Collection5/last_13.txt  \n",
            "  inflating: ./datasets/Collection5/last_14.ann  \n",
            "  inflating: ./datasets/Collection5/last_14.txt  \n",
            "  inflating: ./datasets/Collection5/last_15.ann  \n",
            "  inflating: ./datasets/Collection5/last_15.txt  \n",
            "  inflating: ./datasets/Collection5/last_16.ann  \n",
            "  inflating: ./datasets/Collection5/last_16.txt  \n",
            "  inflating: ./datasets/Collection5/last_17.ann  \n",
            "  inflating: ./datasets/Collection5/last_17.txt  \n",
            "  inflating: ./datasets/Collection5/last_18.ann  \n",
            "  inflating: ./datasets/Collection5/last_18.txt  \n",
            "  inflating: ./datasets/Collection5/last_19.ann  \n",
            "  inflating: ./datasets/Collection5/last_19.txt  \n",
            "  inflating: ./datasets/Collection5/last_20.ann  \n",
            "  inflating: ./datasets/Collection5/last_20.txt  \n",
            "  inflating: ./datasets/Collection5/last_21.ann  \n",
            "  inflating: ./datasets/Collection5/last_21.txt  \n",
            "  inflating: ./datasets/Collection5/last_22.ann  \n",
            "  inflating: ./datasets/Collection5/last_22.txt  \n",
            "  inflating: ./datasets/Collection5/last_23.ann  \n",
            "  inflating: ./datasets/Collection5/last_23.txt  \n",
            "  inflating: ./datasets/Collection5/last_24.ann  \n",
            "  inflating: ./datasets/Collection5/last_24.txt  \n",
            "  inflating: ./datasets/Collection5/last_25.ann  \n",
            "  inflating: ./datasets/Collection5/last_25.txt  \n",
            "  inflating: ./datasets/Collection5/last_26.ann  \n",
            "  inflating: ./datasets/Collection5/last_26.txt  \n",
            "  inflating: ./datasets/Collection5/last_27.ann  \n",
            "  inflating: ./datasets/Collection5/last_27.txt  \n",
            "  inflating: ./datasets/Collection5/last_28.ann  \n",
            "  inflating: ./datasets/Collection5/last_28.txt  \n",
            "  inflating: ./datasets/Collection5/last_29.ann  \n",
            "  inflating: ./datasets/Collection5/last_29.txt  \n",
            "  inflating: ./datasets/Collection5/last_30_new.ann  \n",
            "  inflating: ./datasets/Collection5/last_30_new.txt  \n",
            "  inflating: ./datasets/Collection5/last_31.ann  \n",
            "  inflating: ./datasets/Collection5/last_31.txt  \n",
            "  inflating: ./datasets/Collection5/last_32.ann  \n",
            "  inflating: ./datasets/Collection5/last_32.txt  \n",
            "  inflating: ./datasets/Collection5/last_33.ann  \n",
            "  inflating: ./datasets/Collection5/last_33.txt  \n",
            "  inflating: ./datasets/Collection5/last_34.ann  \n",
            "  inflating: ./datasets/Collection5/last_34.txt  \n",
            "  inflating: ./datasets/Collection5/last_35.ann  \n",
            "  inflating: ./datasets/Collection5/last_35.txt  \n",
            "  inflating: ./datasets/Collection5/last_36.ann  \n",
            "  inflating: ./datasets/Collection5/last_36.txt  \n",
            "  inflating: ./datasets/Collection5/last_37.ann  \n",
            "  inflating: ./datasets/Collection5/last_37.txt  \n",
            "  inflating: ./datasets/Collection5/last_38.ann  \n",
            "  inflating: ./datasets/Collection5/last_38.txt  \n",
            "  inflating: ./datasets/Collection5/last_39.ann  \n",
            "  inflating: ./datasets/Collection5/last_39.txt  \n",
            "  inflating: ./datasets/Collection5/last_40.ann  \n",
            "  inflating: ./datasets/Collection5/last_40.txt  \n",
            "  inflating: ./datasets/Collection5/last_41.ann  \n",
            "  inflating: ./datasets/Collection5/last_41.txt  \n",
            "  inflating: ./datasets/Collection5/last_42.ann  \n",
            "  inflating: ./datasets/Collection5/last_42.txt  \n",
            "  inflating: ./datasets/Collection5/last_43.ann  \n",
            "  inflating: ./datasets/Collection5/last_43.txt  \n",
            "  inflating: ./datasets/Collection5/last_44.ann  \n",
            "  inflating: ./datasets/Collection5/last_44.txt  \n",
            "  inflating: ./datasets/Collection5/last_45.ann  \n",
            "  inflating: ./datasets/Collection5/last_45.txt  \n",
            "  inflating: ./datasets/Collection5/last_46.ann  \n",
            "  inflating: ./datasets/Collection5/last_46.txt  \n",
            "  inflating: ./datasets/Collection5/last_47.ann  \n",
            "  inflating: ./datasets/Collection5/last_47.txt  \n",
            "  inflating: ./datasets/Collection5/last_48.ann  \n",
            "  inflating: ./datasets/Collection5/last_48.txt  \n",
            "  inflating: ./datasets/Collection5/last_49.ann  \n",
            "  inflating: ./datasets/Collection5/last_49.txt  \n",
            "  inflating: ./datasets/Collection5/last_50.ann  \n",
            "  inflating: ./datasets/Collection5/last_50.txt  \n",
            "  inflating: ./datasets/Collection5/last_51.ann  \n",
            "  inflating: ./datasets/Collection5/last_51.txt  \n",
            "  inflating: ./datasets/Collection5/last_52.ann  \n",
            "  inflating: ./datasets/Collection5/last_52.txt  \n",
            "  inflating: ./datasets/Collection5/last_53.ann  \n",
            "  inflating: ./datasets/Collection5/last_53.txt  \n",
            "  inflating: ./datasets/Collection5/last_54.ann  \n",
            "  inflating: ./datasets/Collection5/last_54.txt  \n",
            "  inflating: ./datasets/Collection5/last_55.ann  \n",
            "  inflating: ./datasets/Collection5/last_55.txt  \n",
            "  inflating: ./datasets/Collection5/last_56.ann  \n",
            "  inflating: ./datasets/Collection5/last_56.txt  \n",
            "  inflating: ./datasets/Collection5/last_57.ann  \n",
            "  inflating: ./datasets/Collection5/last_57.txt  \n",
            "  inflating: ./datasets/Collection5/last_58.ann  \n",
            "  inflating: ./datasets/Collection5/last_58.txt  \n",
            "  inflating: ./datasets/Collection5/last_59.ann  \n",
            "  inflating: ./datasets/Collection5/last_59.txt  \n",
            "  inflating: ./datasets/Collection5/last_60.ann  \n",
            "  inflating: ./datasets/Collection5/last_60.txt  \n",
            "  inflating: ./datasets/Collection5/last_61.ann  \n",
            "  inflating: ./datasets/Collection5/last_61.txt  \n",
            "  inflating: ./datasets/Collection5/last_62.ann  \n",
            "  inflating: ./datasets/Collection5/last_62.txt  \n",
            "  inflating: ./datasets/Collection5/last_63.ann  \n",
            "  inflating: ./datasets/Collection5/last_63.txt  \n",
            "  inflating: ./datasets/Collection5/last_64.ann  \n",
            "  inflating: ./datasets/Collection5/last_64.txt  \n",
            "  inflating: ./datasets/Collection5/last_65.ann  \n",
            "  inflating: ./datasets/Collection5/last_65.txt  \n",
            "  inflating: ./datasets/Collection5/last_66.ann  \n",
            "  inflating: ./datasets/Collection5/last_66.txt  \n",
            "  inflating: ./datasets/Collection5/last_67.ann  \n",
            "  inflating: ./datasets/Collection5/last_67.txt  \n",
            "  inflating: ./datasets/Collection5/last_68.ann  \n",
            "  inflating: ./datasets/Collection5/last_68.txt  \n",
            "  inflating: ./datasets/Collection5/last_69.ann  \n",
            "  inflating: ./datasets/Collection5/last_69.txt  \n",
            "  inflating: ./datasets/Collection5/last_70.ann  \n",
            "  inflating: ./datasets/Collection5/last_70.txt  \n",
            "  inflating: ./datasets/Collection5/last_71.ann  \n",
            "  inflating: ./datasets/Collection5/last_71.txt  \n",
            "  inflating: ./datasets/Collection5/last_72.ann  \n",
            "  inflating: ./datasets/Collection5/last_72.txt  \n",
            "  inflating: ./datasets/Collection5/last_73.ann  \n",
            "  inflating: ./datasets/Collection5/last_73.txt  \n",
            "  inflating: ./datasets/Collection5/last_74.ann  \n",
            "  inflating: ./datasets/Collection5/last_74.txt  \n",
            "  inflating: ./datasets/Collection5/last_75.ann  \n",
            "  inflating: ./datasets/Collection5/last_75.txt  \n",
            "  inflating: ./datasets/Collection5/lenoblast.ann  \n",
            "  inflating: ./datasets/Collection5/lenoblast.txt  \n",
            "  inflating: ./datasets/Collection5/maykl dzhekson.ann  \n",
            "  inflating: ./datasets/Collection5/maykl dzhekson.txt  \n",
            "  inflating: ./datasets/Collection5/mvd.ann  \n",
            "  inflating: ./datasets/Collection5/mvd.txt  \n",
            "  inflating: ./datasets/Collection5/mvd2.ann  \n",
            "  inflating: ./datasets/Collection5/mvd2.txt  \n",
            "  inflating: ./datasets/Collection5/rosobrnadzor.ann  \n",
            "  inflating: ./datasets/Collection5/rosobrnadzor.txt  \n",
            "  inflating: ./datasets/Collection5/ryadovoy chelah.ann  \n",
            "  inflating: ./datasets/Collection5/ryadovoy chelah.txt  \n",
            "  inflating: ./datasets/Collection5/semenenko.ann  \n",
            "  inflating: ./datasets/Collection5/semenenko.txt  \n",
            "  inflating: ./datasets/Collection5/shojgu1.ann  \n",
            "  inflating: ./datasets/Collection5/shojgu1.txt  \n",
            "  inflating: ./datasets/Collection5/shojgu3.ann  \n",
            "  inflating: ./datasets/Collection5/shojgu3.txt  \n",
            "  inflating: ./datasets/Collection5/shojgu4.ann  \n",
            "  inflating: ./datasets/Collection5/shojgu4.txt  \n",
            "  inflating: ./datasets/Collection5/shojgu6.ann  \n",
            "  inflating: ./datasets/Collection5/shojgu6.txt  \n",
            "  inflating: ./datasets/Collection5/si_tzjanpin.ann  \n",
            "  inflating: ./datasets/Collection5/si_tzjanpin.txt  \n",
            "  inflating: ./datasets/Collection5/sobjanin2.ann  \n",
            "  inflating: ./datasets/Collection5/sobjanin2.txt  \n",
            "  inflating: ./datasets/Collection5/turkmenija.ann  \n",
            "  inflating: ./datasets/Collection5/turkmenija.txt  \n",
            "  inflating: ./datasets/Collection5/uchitel.ann  \n",
            "  inflating: ./datasets/Collection5/uchitel.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SynTagRus is now split into 3 parts. Download them all\n",
        "!wget -O ./datasets/ru_syntagrus-ud-train-a.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train-a.conllu\n",
        "!wget -O ./datasets/ru_syntagrus-ud-train-b.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train-b.conllu\n",
        "!wget -O ./datasets/ru_syntagrus-ud-train-c.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train-c.conllu\n",
        "# Concatenate files together to get the full training set\n",
        "!cat ./datasets/ru_syntagrus-ud-train-a.conllu ./datasets/ru_syntagrus-ud-train-b.conllu ./datasets/ru_syntagrus-ud-train-c.conllu > ./datasets/ru_syntagrus-ud-train.conllu\n",
        "!wget -O ./datasets/ru_syntagrus-ud-dev.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-dev.conllu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5wbDjaBREOJ",
        "outputId": "34f86af6-5f46-4fa5-d1ca-3b83db52102a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-08-04 12:29:25--  https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train-a.conllu\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 40736581 (39M) [text/plain]\n",
            "Saving to: ‘./datasets/ru_syntagrus-ud-train-a.conllu’\n",
            "\n",
            "./datasets/ru_synta 100%[===================>]  38.85M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2023-08-04 12:29:28 (354 MB/s) - ‘./datasets/ru_syntagrus-ud-train-a.conllu’ saved [40736581/40736581]\n",
            "\n",
            "--2023-08-04 12:29:28--  https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train-b.conllu\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 42819832 (41M) [text/plain]\n",
            "Saving to: ‘./datasets/ru_syntagrus-ud-train-b.conllu’\n",
            "\n",
            "./datasets/ru_synta 100%[===================>]  40.84M   265MB/s    in 0.2s    \n",
            "\n",
            "2023-08-04 12:29:32 (265 MB/s) - ‘./datasets/ru_syntagrus-ud-train-b.conllu’ saved [42819832/42819832]\n",
            "\n",
            "--2023-08-04 12:29:32--  https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train-c.conllu\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 32367510 (31M) [text/plain]\n",
            "Saving to: ‘./datasets/ru_syntagrus-ud-train-c.conllu’\n",
            "\n",
            "./datasets/ru_synta 100%[===================>]  30.87M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2023-08-04 12:29:34 (279 MB/s) - ‘./datasets/ru_syntagrus-ud-train-c.conllu’ saved [32367510/32367510]\n",
            "\n",
            "--2023-08-04 12:29:35--  https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-dev.conllu\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14704579 (14M) [text/plain]\n",
            "Saving to: ‘./datasets/ru_syntagrus-ud-dev.conllu’\n",
            "\n",
            "./datasets/ru_synta 100%[===================>]  14.02M  67.1MB/s    in 0.2s    \n",
            "\n",
            "2023-08-04 12:29:37 (67.1 MB/s) - ‘./datasets/ru_syntagrus-ud-dev.conllu’ saved [14704579/14704579]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellId": "ajqymqxwt3uihgi565luy",
        "id": "3d74446b",
        "outputId": "813d9e5d-8130-45d2-fde7-a9e982e61ab0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import matplotlib\n",
        "%matplotlib inline\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import pyconll\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.tag import DefaultTagger, UnigramTagger, BigramTagger, TrigramTagger\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "import os\n",
        "import corus\n",
        "import deeppavlov\n",
        "from deeppavlov import configs, build_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellId": "v7zwlwo7xrlqop9t2l2psd",
        "id": "f8f1bb88"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "import pandas as pd\n",
        "from navec import Navec\n",
        "from slovnet import NER\n",
        "from ipymarkup import show_span_ascii_markup as show_markup\n",
        "from razdel import tokenize\n",
        "import sklearn_crfsuite\n",
        "from sklearn_crfsuite import scorers\n",
        "from sklearn_crfsuite import metrics\n",
        "from sklearn.metrics import f1_score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Задание 1."
      ],
      "metadata": {
        "id": "auz-0rdwNaCc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8x7-5rWeI6wf"
      },
      "outputs": [],
      "source": [
        "full_train = pyconll.load_from_file('datasets/ru_syntagrus-ud-train.conllu')\n",
        "full_test = pyconll.load_from_file('datasets/ru_syntagrus-ud-dev.conllu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8blLBYGW7bl"
      },
      "outputs": [],
      "source": [
        "fdata_train = []\n",
        "for sent in full_train[:]:\n",
        "    fdata_train.append([(token.form, token.upos) for token in sent])\n",
        "\n",
        "fdata_test = []\n",
        "for sent in full_test[:]:\n",
        "    fdata_test.append([(token.form, token.upos) for token in sent])\n",
        "\n",
        "fdata_sent_test = []\n",
        "for sent in full_test[:]:\n",
        "    fdata_sent_test.append([token.form for token in sent])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKm2BHoyW_Cf"
      },
      "outputs": [],
      "source": [
        "all_train_texts = [' '.join(token.form if token.form is not None  else '' for token in sent) for sent in full_train]\n",
        "all_test_texts = [' '.join(token.form if token.form is not None  else '' for token in sent) for sent in full_test]\n",
        "\n",
        "all_train_labels = [' '.join(token.form if token.form is not None  else '' for token in sent) for sent in full_train]\n",
        "all_test_labels = [' '.join(token.form if token.form is not None  else '' for token in sent) for sent in full_test]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8a4CePtbSz5"
      },
      "outputs": [],
      "source": [
        "test_sent = all_test_texts[20]\n",
        "test_data = fdata_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "lsYBPeG-bxl-",
        "outputId": "f0b19e59-d08e-4683-a4d4-0bb0e69d23fd"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[('таким', 'DET'),\n",
              " ('образом', 'NOUN'),\n",
              " (',', 'PUNCT'),\n",
              " ('алгоритм', 'NOUN'),\n",
              " ('выдаёт', None),\n",
              " ('один', 'NUM'),\n",
              " ('и', 'CCONJ'),\n",
              " ('тот', 'DET'),\n",
              " ('же', 'PART'),\n",
              " ('результат', 'NOUN'),\n",
              " ('(', 'PUNCT'),\n",
              " ('ответ', 'NOUN'),\n",
              " (')', 'PUNCT'),\n",
              " ('для', 'ADP'),\n",
              " ('одних', 'DET'),\n",
              " ('и', 'CCONJ'),\n",
              " ('тех', 'DET'),\n",
              " ('же', 'PART'),\n",
              " ('исходных', 'ADJ'),\n",
              " ('данных', 'NOUN'),\n",
              " ('.', 'PUNCT')]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0.8782863467673677"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "unigram_tagger = UnigramTagger(fdata_train)\n",
        "display(unigram_tagger.tag(test_sent.lower().split()), unigram_tagger.evaluate(test_data))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_tagger = BigramTagger(fdata_train)\n",
        "display(bigram_tagger.tag(test_sent.lower().split()), bigram_tagger.evaluate(test_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "S-ibuTS9UOq5",
        "outputId": "dfdb4c0c-1090-420b-cdb6-60611764ba21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[('таким', None),\n",
              " ('образом', None),\n",
              " (',', 'PUNCT'),\n",
              " ('алгоритм', 'NOUN'),\n",
              " ('выдаёт', None),\n",
              " ('один', 'NUM'),\n",
              " ('и', 'CCONJ'),\n",
              " ('тот', 'DET'),\n",
              " ('же', 'PART'),\n",
              " ('результат', 'NOUN'),\n",
              " ('(', 'PUNCT'),\n",
              " ('ответ', 'NOUN'),\n",
              " (')', 'PUNCT'),\n",
              " ('для', 'ADP'),\n",
              " ('одних', 'DET'),\n",
              " ('и', 'CCONJ'),\n",
              " ('тех', 'DET'),\n",
              " ('же', 'PART'),\n",
              " ('исходных', None),\n",
              " ('данных', None),\n",
              " ('.', 'PUNCT')]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0.7101308678950452"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "JsR9cv6udH-2",
        "outputId": "b7d335f4-9b43-4fe3-8d5f-5ff182dfe113"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[('таким', 'DET'),\n",
              " ('образом', 'NOUN'),\n",
              " (',', 'PUNCT'),\n",
              " ('алгоритм', 'NOUN'),\n",
              " ('выдаёт', None),\n",
              " ('один', 'NUM'),\n",
              " ('и', 'CCONJ'),\n",
              " ('тот', 'DET'),\n",
              " ('же', 'PART'),\n",
              " ('результат', 'NOUN'),\n",
              " ('(', 'PUNCT'),\n",
              " ('ответ', 'NOUN'),\n",
              " (')', 'PUNCT'),\n",
              " ('для', 'ADP'),\n",
              " ('одних', 'DET'),\n",
              " ('и', 'CCONJ'),\n",
              " ('тех', 'DET'),\n",
              " ('же', 'PART'),\n",
              " ('исходных', 'ADJ'),\n",
              " ('данных', 'NOUN'),\n",
              " ('.', 'PUNCT')]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0.8839768214076438"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "bigram_tagger = BigramTagger(fdata_train, backoff=unigram_tagger)\n",
        "display(bigram_tagger.tag(test_sent.lower().split()), bigram_tagger.evaluate(test_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "aJjk3uD-GnO5",
        "outputId": "90886507-4382-4a30-92ac-b3246392281a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[('таким', None),\n",
              " ('образом', None),\n",
              " (',', 'PUNCT'),\n",
              " ('алгоритм', None),\n",
              " ('выдаёт', None),\n",
              " ('один', None),\n",
              " ('и', 'PART'),\n",
              " ('тот', None),\n",
              " ('же', None),\n",
              " ('результат', None),\n",
              " ('(', None),\n",
              " ('ответ', None),\n",
              " (')', None),\n",
              " ('для', 'ADP'),\n",
              " ('одних', None),\n",
              " ('и', None),\n",
              " ('тех', None),\n",
              " ('же', None),\n",
              " ('исходных', None),\n",
              " ('данных', None),\n",
              " ('.', 'PUNCT')]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0.4067191874470994"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "trigram_tagger = TrigramTagger(fdata_train)\n",
        "display(trigram_tagger.tag(test_sent.lower().split()), trigram_tagger.evaluate(test_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "RDkkFS-ZdaNT",
        "outputId": "17fce284-b07f-4b4f-96f8-ca993ffa4ecc"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[('таким', 'DET'),\n",
              " ('образом', 'NOUN'),\n",
              " (',', 'PUNCT'),\n",
              " ('алгоритм', 'NOUN'),\n",
              " ('выдаёт', None),\n",
              " ('один', 'NUM'),\n",
              " ('и', 'CCONJ'),\n",
              " ('тот', 'DET'),\n",
              " ('же', 'PART'),\n",
              " ('результат', 'NOUN'),\n",
              " ('(', 'PUNCT'),\n",
              " ('ответ', 'NOUN'),\n",
              " (')', 'PUNCT'),\n",
              " ('для', 'ADP'),\n",
              " ('одних', 'DET'),\n",
              " ('и', 'CCONJ'),\n",
              " ('тех', 'DET'),\n",
              " ('же', 'PART'),\n",
              " ('исходных', 'ADJ'),\n",
              " ('данных', 'NOUN'),\n",
              " ('.', 'PUNCT')]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0.8830522820496126"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "trigram_tagger = TrigramTagger(fdata_train, backoff=bigram_tagger)\n",
        "display(trigram_tagger.tag(test_sent.lower().split()), trigram_tagger.evaluate(test_data))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def backoff_tagger(train_sents, tagger_classes, backoff=None):\n",
        "    for cls in tagger_classes:\n",
        "        backoff = cls(train_sents, backoff=backoff)\n",
        "    return backoff\n",
        "\n",
        "\n",
        "backoff = DefaultTagger('NN')\n",
        "tag = backoff_tagger(fdata_train,\n",
        "                     [UnigramTagger, BigramTagger, TrigramTagger],\n",
        "                     backoff = backoff)\n"
      ],
      "metadata": {
        "id": "FI3ZQJArTgQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tag.evaluate(test_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6pKwggL8T0gB",
        "outputId": "b4008540-379b-47f1-e708-b6794bedbf87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8825379256462009"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Комбинация тэггеров дает результат лучше, чем UnigramTagger, BigramTagger, TrigramTagger по отдельности. \\\n",
        "Лучший результат показал BigramTagger, у которого в качестве backoff был UnigramTagger."
      ],
      "metadata": {
        "id": "ClQ8i7pgUnUX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellId": "6ilvx0n7gfefiu3ltggnvf",
        "id": "45f76fde"
      },
      "outputs": [],
      "source": [
        "train_tok = []\n",
        "train_label = []\n",
        "for sent in fdata_train[:]:\n",
        "    for tok in sent:\n",
        "        if (tok is None or tok[0] is None):\n",
        "          continue\n",
        "        train_tok.append(tok[0])\n",
        "        train_label.append('NO_TAG' if tok[1] is None else tok[1])\n",
        "\n",
        "test_tok = []\n",
        "test_label = []\n",
        "for sent in fdata_test[:]:\n",
        "    for tok in sent:\n",
        "        if (tok is None or tok[0] is None):\n",
        "          continue\n",
        "        test_tok.append(tok[0])\n",
        "        test_label.append('NO_TAG' if tok[1] is None else tok[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellId": "03uiu8yo0vr1q54401q6fq",
        "id": "876bccfb"
      },
      "outputs": [],
      "source": [
        "le = LabelEncoder()\n",
        "train_enc_labels = le.fit_transform(train_label)\n",
        "test_enc_labels = le.transform(test_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellId": "sgg0r5cvlkqobd77dlwa3k",
        "id": "d8bb7fa3"
      },
      "outputs": [],
      "source": [
        "vectorizers = [CountVectorizer(ngram_range=(1, 3), analyzer='char'),\n",
        "               TfidfVectorizer(ngram_range=(1, 3), analyzer='char'),\n",
        "               HashingVectorizer(ngram_range=(1, 3), analyzer='char', n_features=1000)]\n",
        "vectorizers_word = [CountVectorizer(ngram_range=(1, 3), analyzer='word'),\n",
        "               TfidfVectorizer(ngram_range=(1, 3), analyzer='word'),\n",
        "               HashingVectorizer(ngram_range=(1, 3), analyzer='word', n_features=1000)]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f1_scores = []\n",
        "accuracy_scores = []\n",
        "\n",
        "for vectorizer in vectorizers + vectorizers_word:\n",
        "    X_train = vectorizer.fit_transform(train_tok)\n",
        "    X_test = vectorizer.transform(test_tok)\n",
        "\n",
        "    lr = LogisticRegression(random_state=0, max_iter=100)\n",
        "    lr.fit(X_train, train_enc_labels)\n",
        "    pred = lr.predict(X_test)\n",
        "    f1 = f1_score(test_enc_labels, pred, average='weighted')\n",
        "    f1_scores.append(f1)\n",
        "    acc = accuracy_score(test_enc_labels, pred)\n",
        "    accuracy_scores.append(acc)\n",
        "\n",
        "    print(vectorizer)\n",
        "    print(classification_report(test_enc_labels, pred, target_names=le.classes_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkyguniDW6g_",
        "outputId": "7a6cfab0-4592-4657-e8cf-a1d083d36439"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CountVectorizer(analyzer='char', ngram_range=(1, 3))\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ADJ       0.91      0.91      0.91     15103\n",
            "         ADP       0.99      1.00      0.99     13717\n",
            "         ADV       0.90      0.89      0.90      7783\n",
            "         AUX       0.81      0.97      0.88      1390\n",
            "       CCONJ       0.88      0.98      0.93      5672\n",
            "         DET       0.83      0.78      0.80      4265\n",
            "        INTJ       0.28      0.29      0.29        24\n",
            "        NOUN       0.92      0.95      0.94     36238\n",
            "      NO_TAG       1.00      1.00      1.00       204\n",
            "         NUM       0.87      0.89      0.88      1734\n",
            "        PART       0.94      0.77      0.85      5125\n",
            "        PRON       0.86      0.88      0.87      7444\n",
            "       PROPN       0.78      0.58      0.67      5473\n",
            "       PUNCT       1.00      1.00      1.00     29186\n",
            "       SCONJ       0.81      0.90      0.85      2865\n",
            "         SYM       1.00      0.85      0.92        62\n",
            "        VERB       0.94      0.94      0.94     17110\n",
            "           X       0.36      0.19      0.25       134\n",
            "\n",
            "    accuracy                           0.93    153529\n",
            "   macro avg       0.84      0.82      0.83    153529\n",
            "weighted avg       0.93      0.93      0.93    153529\n",
            "\n",
            "TfidfVectorizer(analyzer='char', ngram_range=(1, 3))\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ADJ       0.90      0.90      0.90     15103\n",
            "         ADP       0.99      0.99      0.99     13717\n",
            "         ADV       0.90      0.86      0.88      7783\n",
            "         AUX       0.82      0.96      0.88      1390\n",
            "       CCONJ       0.89      0.98      0.93      5672\n",
            "         DET       0.81      0.81      0.81      4265\n",
            "        INTJ       0.00      0.00      0.00        24\n",
            "        NOUN       0.91      0.95      0.93     36238\n",
            "      NO_TAG       1.00      1.00      1.00       204\n",
            "         NUM       0.84      0.90      0.87      1734\n",
            "        PART       0.95      0.77      0.85      5125\n",
            "        PRON       0.87      0.86      0.87      7444\n",
            "       PROPN       0.76      0.57      0.65      5473\n",
            "       PUNCT       1.00      1.00      1.00     29186\n",
            "       SCONJ       0.80      0.92      0.86      2865\n",
            "         SYM       1.00      0.85      0.92        62\n",
            "        VERB       0.93      0.93      0.93     17110\n",
            "           X       0.34      0.27      0.30       134\n",
            "\n",
            "    accuracy                           0.92    153529\n",
            "   macro avg       0.82      0.81      0.81    153529\n",
            "weighted avg       0.92      0.92      0.92    153529\n",
            "\n",
            "HashingVectorizer(analyzer='char', n_features=1000, ngram_range=(1, 3))\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ADJ       0.83      0.83      0.83     15103\n",
            "         ADP       0.98      0.99      0.98     13717\n",
            "         ADV       0.82      0.78      0.80      7783\n",
            "         AUX       0.81      0.96      0.88      1390\n",
            "       CCONJ       0.88      0.98      0.93      5672\n",
            "         DET       0.90      0.65      0.75      4265\n",
            "        INTJ       0.00      0.00      0.00        24\n",
            "        NOUN       0.84      0.90      0.87     36238\n",
            "      NO_TAG       1.00      1.00      1.00       204\n",
            "         NUM       0.80      0.80      0.80      1734\n",
            "        PART       0.93      0.73      0.82      5125\n",
            "        PRON       0.79      0.93      0.85      7444\n",
            "       PROPN       0.68      0.42      0.52      5473\n",
            "       PUNCT       1.00      1.00      1.00     29186\n",
            "       SCONJ       0.77      0.91      0.84      2865\n",
            "         SYM       0.98      0.85      0.91        62\n",
            "        VERB       0.87      0.84      0.86     17110\n",
            "           X       0.30      0.19      0.23       134\n",
            "\n",
            "    accuracy                           0.88    153529\n",
            "   macro avg       0.79      0.76      0.77    153529\n",
            "weighted avg       0.88      0.88      0.88    153529\n",
            "\n",
            "CountVectorizer(ngram_range=(1, 3))\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ADJ       0.94      0.40      0.56     15103\n",
            "         ADP       0.98      0.48      0.64     13717\n",
            "         ADV       0.94      0.79      0.86      7783\n",
            "         AUX       0.78      0.90      0.83      1390\n",
            "       CCONJ       0.88      0.20      0.33      5672\n",
            "         DET       0.91      0.63      0.74      4265\n",
            "        INTJ       0.00      0.00      0.00        24\n",
            "        NOUN       0.98      0.68      0.81     36238\n",
            "      NO_TAG       0.00      0.00      0.00       204\n",
            "         NUM       0.86      0.62      0.72      1734\n",
            "        PART       0.98      0.74      0.85      5125\n",
            "        PRON       0.83      0.82      0.82      7444\n",
            "       PROPN       0.89      0.15      0.26      5473\n",
            "       PUNCT       0.38      1.00      0.55     29186\n",
            "       SCONJ       0.75      0.97      0.84      2865\n",
            "         SYM       0.00      0.00      0.00        62\n",
            "        VERB       0.98      0.45      0.62     17110\n",
            "           X       0.00      0.00      0.00       134\n",
            "\n",
            "    accuracy                           0.65    153529\n",
            "   macro avg       0.67      0.49      0.52    153529\n",
            "weighted avg       0.83      0.65      0.66    153529\n",
            "\n",
            "TfidfVectorizer(ngram_range=(1, 3))\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ADJ       0.94      0.39      0.55     15103\n",
            "         ADP       0.98      0.48      0.64     13717\n",
            "         ADV       0.95      0.76      0.84      7783\n",
            "         AUX       0.81      0.94      0.87      1390\n",
            "       CCONJ       0.88      0.20      0.33      5672\n",
            "         DET       0.84      0.71      0.77      4265\n",
            "        INTJ       0.00      0.00      0.00        24\n",
            "        NOUN       0.98      0.68      0.81     36238\n",
            "      NO_TAG       0.00      0.00      0.00       204\n",
            "         NUM       0.86      0.59      0.70      1734\n",
            "        PART       0.97      0.74      0.84      5125\n",
            "        PRON       0.88      0.78      0.83      7444\n",
            "       PROPN       0.91      0.14      0.25      5473\n",
            "       PUNCT       0.37      1.00      0.54     29186\n",
            "       SCONJ       0.75      0.97      0.85      2865\n",
            "         SYM       0.00      0.00      0.00        62\n",
            "        VERB       0.99      0.45      0.62     17110\n",
            "           X       0.00      0.00      0.00       134\n",
            "\n",
            "    accuracy                           0.65    153529\n",
            "   macro avg       0.67      0.49      0.52    153529\n",
            "weighted avg       0.83      0.65      0.66    153529\n",
            "\n",
            "HashingVectorizer(n_features=1000, ngram_range=(1, 3))\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ADJ       0.45      0.17      0.25     15103\n",
            "         ADP       0.82      0.47      0.60     13717\n",
            "         ADV       0.58      0.62      0.60      7783\n",
            "         AUX       0.70      0.94      0.80      1390\n",
            "       CCONJ       0.84      0.18      0.30      5672\n",
            "         DET       0.50      0.58      0.54      4265\n",
            "        INTJ       0.00      0.00      0.00        24\n",
            "        NOUN       0.26      0.55      0.35     36238\n",
            "      NO_TAG       0.00      0.00      0.00       204\n",
            "         NUM       0.41      0.45      0.43      1734\n",
            "        PART       0.85      0.74      0.79      5125\n",
            "        PRON       0.65      0.76      0.70      7444\n",
            "       PROPN       0.38      0.07      0.12      5473\n",
            "       PUNCT       0.00      0.00      0.00     29186\n",
            "       SCONJ       0.70      0.90      0.79      2865\n",
            "         SYM       0.00      0.00      0.00        62\n",
            "        VERB       0.48      0.23      0.31     17110\n",
            "           X       0.00      0.00      0.00       134\n",
            "\n",
            "    accuracy                           0.36    153529\n",
            "   macro avg       0.42      0.37      0.37    153529\n",
            "weighted avg       0.40      0.36      0.34    153529\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellId": "i6iaszvctaehiivb1chohl",
        "id": "a08fcb01",
        "outputId": "f0ceeb47-23a0-4949-d409-33ba76831974",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                          Vectorizer  f1_score\n",
              "0  CountVectorizer(analyzer='char', ngram_range=(...  0.927277\n",
              "1  TfidfVectorizer(analyzer='char', ngram_range=(...  0.921265\n",
              "2  HashingVectorizer(analyzer='char', n_features=...  0.877628\n",
              "3                CountVectorizer(ngram_range=(1, 3))  0.661288\n",
              "4                TfidfVectorizer(ngram_range=(1, 3))  0.659126\n",
              "5  HashingVectorizer(n_features=1000, ngram_range...  0.343001"
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-540bf7a1-a1fb-4187-8fdd-995c9ed1abec\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Vectorizer</th>\n",
              "      <th>f1_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>CountVectorizer(analyzer='char', ngram_range=(...</td>\n",
              "      <td>0.927277</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>TfidfVectorizer(analyzer='char', ngram_range=(...</td>\n",
              "      <td>0.921265</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>HashingVectorizer(analyzer='char', n_features=...</td>\n",
              "      <td>0.877628</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>CountVectorizer(ngram_range=(1, 3))</td>\n",
              "      <td>0.661288</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>TfidfVectorizer(ngram_range=(1, 3))</td>\n",
              "      <td>0.659126</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>HashingVectorizer(n_features=1000, ngram_range...</td>\n",
              "      <td>0.343001</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-540bf7a1-a1fb-4187-8fdd-995c9ed1abec')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-f27dd060-30be-4902-a557-17ad2453006c\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f27dd060-30be-4902-a557-17ad2453006c')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-f27dd060-30be-4902-a557-17ad2453006c button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-540bf7a1-a1fb-4187-8fdd-995c9ed1abec button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-540bf7a1-a1fb-4187-8fdd-995c9ed1abec');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "result_model = pd.DataFrame({'Vectorizer': vectorizers + vectorizers_word,\n",
        "                            'f1_score': f1_scores})\n",
        "result_model.sort_values('f1_score', ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellId": "r3zu97scrosk241qqrxxpq",
        "id": "b780f1d7",
        "outputId": "ae20b878-60b0-4319-c97d-05a3e1db0882",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                          Vectorizer  Accuracy\n",
              "0  CountVectorizer(analyzer='char', ngram_range=(...  0.928880\n",
              "1  TfidfVectorizer(analyzer='char', ngram_range=(...  0.922855\n",
              "2  HashingVectorizer(analyzer='char', n_features=...  0.880948\n",
              "3                CountVectorizer(ngram_range=(1, 3))  0.652483\n",
              "4                TfidfVectorizer(ngram_range=(1, 3))  0.649740\n",
              "5  HashingVectorizer(n_features=1000, ngram_range...  0.363710"
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-9858ea29-7d81-4736-9f3b-639307594625\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Vectorizer</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>CountVectorizer(analyzer='char', ngram_range=(...</td>\n",
              "      <td>0.928880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>TfidfVectorizer(analyzer='char', ngram_range=(...</td>\n",
              "      <td>0.922855</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>HashingVectorizer(analyzer='char', n_features=...</td>\n",
              "      <td>0.880948</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>CountVectorizer(ngram_range=(1, 3))</td>\n",
              "      <td>0.652483</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>TfidfVectorizer(ngram_range=(1, 3))</td>\n",
              "      <td>0.649740</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>HashingVectorizer(n_features=1000, ngram_range...</td>\n",
              "      <td>0.363710</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9858ea29-7d81-4736-9f3b-639307594625')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-182eeb40-dc2e-4723-9e34-beddf2f2c8d0\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-182eeb40-dc2e-4723-9e34-beddf2f2c8d0')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-182eeb40-dc2e-4723-9e34-beddf2f2c8d0 button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9858ea29-7d81-4736-9f3b-639307594625 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9858ea29-7d81-4736-9f3b-639307594625');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "result_model_acc = pd.DataFrame({'Vectorizer': vectorizers + vectorizers_word,\n",
        "                            'Accuracy': accuracy_scores})\n",
        "result_model_acc.sort_values('Accuracy', ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Логистическая регрессия работает заметно медленнее, но показывает большую точность."
      ],
      "metadata": {
        "id": "Lmzd_xw8m_Au"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Задание 2."
      ],
      "metadata": {
        "id": "2GGHM106UwJf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_for_nltk = []\n",
        "\n",
        "for filename in os.scandir(\"datasets/Collection5/\"):\n",
        "    if filename.is_file():\n",
        "        name, ext = os.path.splitext(filename.path)\n",
        "        if ext == \".txt\":\n",
        "          with(open(filename.path,\"r\",encoding='utf-8') as f):\n",
        "            text = f.read()\n",
        "            tokens = text.split()\n",
        "\n",
        "          tagged_tokens = {}\n",
        "\n",
        "          with(open(name + \".ann\",\"r\",encoding='utf-8') as f):\n",
        "            for line in f:\n",
        "              parts = line.split()\n",
        "              tag = parts[1]\n",
        "              name = ' '.join(parts[4:])\n",
        "              tagged_tokens[name] = tag\n",
        "\n",
        "          dataset_for_nltk.append((text, tagged_tokens))\n"
      ],
      "metadata": {
        "id": "U399h4WsqEsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0MV_Ha37Pyqt",
        "outputId": "27ae36c4-6e0b-4a54-c0a2-62ba4fae56f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Лукашенко': 'PER', '\"Белорусской калийной компанией\"': 'ORG', 'Александр Лукашенко': 'PER', 'Администрации': 'ORG', 'ЗАО': 'ORG', 'Белоруссии': 'GEOPOLIT', 'Валерия Иванова': 'PER', 'Администрации президента': 'ORG', 'Елены Кудрявец': 'PER', '\"Белорусская калийная компания\"': 'ORG', 'БКК': 'ORG', 'Иванов': 'PER', 'Кудрявец': 'PER', 'Белорусской калийной компании': 'ORG', 'Российской Федерации': 'GEOPOLIT', '\"Беларуськалия\"': 'ORG', '\"Уралкалия\"': 'ORG', 'ОАО \"Уралкалий\"': 'ORG', 'ОАО \"Беларуськалий\"': 'ORG', 'ЗАО \"Белорусская калийная компания\"': 'ORG', 'ОАО \"Банк развития Республики Беларусь\"': 'ORG', 'Сергей Румас': 'PER', 'КГБ': 'ORG', 'Валерия Вакульчика': 'PER', 'Виктора Лукашенко': 'PER', '\"БКК\"': 'ORG', 'Сулейманом Керимовым': 'PER', 'Керимов': 'PER', 'Керимовым': 'PER', 'Минске': 'LOC', 'России': 'GEOPOLIT', '\"Уралкалию\"': 'ORG', '\"Беларуськалию\"': 'ORG', 'ИА REGNUM': 'MEDIA', '\"Союзкалий\"': 'ORG', 'Швейцарии': 'GEOPOLIT', '\"Международную калийную компанию\"': 'ORG', 'Soyuzkali GmbH (\"Союзкалий\")': 'ORG', 'Совмина': 'ORG', '\"Уралкалий\"': 'ORG'}\n",
            "[('Лукашенко', 'GPE'), ('Александр', 'PERSON'), ('Валерия Иванова', 'PERSON'), ('Валерия Иванова', 'PERSON'), ('Иванов', 'PERSON'), ('Лукашенко', 'PERSON'), ('Вы', 'PERSON'), ('Более', 'PERSON'), ('Вы', 'PERSON'), ('Иванова мы', 'PERSON'), ('Поэтому', 'PERSON'), ('Надо', 'PERSON'), ('Отметим', 'PERSON'), ('Кроме', 'PERSON'), ('Банк', 'PERSON'), ('Сергей Румас', 'PERSON'), ('Валерия Вакульчика', 'PERSON'), ('Виктора Лукашенко', 'PERSON'), ('Лукашенко', 'PERSON'), ('Минске', 'PERSON'), ('России', 'PERSON'), ('БКК', 'ORGANIZATION'), ('БКК', 'ORGANIZATION'), ('Совмина Белоруссии', 'PERSON')]\n"
          ]
        }
      ],
      "source": [
        "for entry in dataset_for_nltk:\n",
        "  pred = [(' '.join(c[0] for c in chunk), chunk.label() ) for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(entry[0]))) if hasattr(chunk, 'label')]\n",
        "  print(entry[1])\n",
        "  print(pred)\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download ru_core_news_lg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7EB08mCkq9lo",
        "outputId": "14944d91-85ea-4201-be88-42aa48040a22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-08-04 13:10:41.737417: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting ru-core-news-lg==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/ru_core_news_lg-3.6.0/ru_core_news_lg-3.6.0-py3-none-any.whl (513.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m513.4/513.4 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ru-core-news-lg==3.6.0) (3.6.0)\n",
            "Collecting pymorphy3>=1.0.0 (from ru-core-news-lg==3.6.0)\n",
            "  Downloading pymorphy3-1.2.0-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dawg-python>=0.7.1 (from pymorphy3>=1.0.0->ru-core-news-lg==3.6.0)\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Collecting docopt>=0.6 (from pymorphy3>=1.0.0->ru-core-news-lg==3.6.0)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pymorphy3-dicts-ru (from pymorphy3>=1.0.0->ru-core-news-lg==3.6.0)\n",
            "  Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m68.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->ru-core-news-lg==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->ru-core-news-lg==3.6.0) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->ru-core-news-lg==3.6.0) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->ru-core-news-lg==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->ru-core-news-lg==3.6.0) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->ru-core-news-lg==3.6.0) (8.1.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->ru-core-news-lg==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->ru-core-news-lg==3.6.0) (2.4.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->ru-core-news-lg==3.6.0) (2.0.9)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->ru-core-news-lg==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->ru-core-news-lg==3.6.0) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->ru-core-news-lg==3.6.0) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->ru-core-news-lg==3.6.0) (4.64.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->ru-core-news-lg==3.6.0) (1.22.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->ru-core-news-lg==3.6.0) (2.27.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->ru-core-news-lg==3.6.0) (1.10.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->ru-core-news-lg==3.6.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->ru-core-news-lg==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->ru-core-news-lg==3.6.0) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->ru-core-news-lg==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.7.0,>=3.6.0->ru-core-news-lg==3.6.0) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->ru-core-news-lg==3.6.0) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->ru-core-news-lg==3.6.0) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->ru-core-news-lg==3.6.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->ru-core-news-lg==3.6.0) (3.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->ru-core-news-lg==3.6.0) (0.7.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->ru-core-news-lg==3.6.0) (0.1.0)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->ru-core-news-lg==3.6.0) (8.1.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->ru-core-news-lg==3.6.0) (2.1.3)\n",
            "Building wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13705 sha256=6c837a068e2cb0a968e5156756d05549328cda9137d6e6b2a75719196407393d\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built docopt\n",
            "Installing collected packages: pymorphy3-dicts-ru, docopt, dawg-python, pymorphy3, ru-core-news-lg\n",
            "Successfully installed dawg-python-0.7.2 docopt-0.6.2 pymorphy3-1.2.0 pymorphy3-dicts-ru-2.4.417150.4580142 ru-core-news-lg-3.6.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('ru_core_news_lg')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Bh0o4NPAWK20",
        "outputId": "76f8b87e-82b3-4949-cab3-4a7f8cd1d0b3"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\"></br>\n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Лукашенко\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
              "</mark>\n",
              ": &quot;\n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Белорусской калийной компанией\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              "&quot; руководил некомпетентный чиновник</br></br>\n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Александр Лукашенко\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
              "</mark>\n",
              " 25 июля рассмотрел кадровые вопросы, назначив нового замглавы своей \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Администрации\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " и нового руководителя спецэкспортёра белорусских и российских калийных удобрений \n",
              "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    ЗАО\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
              "</mark>\n",
              ", сообщает пресс-служба президента \n",
              "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Белоруссии\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
              "</mark>\n",
              ".</br></br>В частности, руководитель республики назначил \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Валерия Иванова\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
              "</mark>\n",
              " заместителем главы \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Администрации президента\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              ". Он также дал согласие на назначение \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Елены Кудрявец\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
              "</mark>\n",
              " генеральным директором закрытого акционерного общества &quot;\n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Белорусская калийная компания\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              "&quot; - на этой должности она сменила \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Валерия Иванова\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
              "</mark>\n",
              ", ушедшего на повышение (до назначения председателем \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    БКК\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Иванов\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
              "</mark>\n",
              " был зампредом правительства \n",
              "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Белоруссии\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
              "</mark>\n",
              ").</br></br>Обращаясь к \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Кудрявец\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
              "</mark>\n",
              ", \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Лукашенко\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
              "</mark>\n",
              " отметил, что ситуация в калийной отрасли - &quot;это не только сфера калийных удобрений, это сфера торговли, притом в глобальном масштабе. Это не то, что пришел, где-то прикупил чего-то, потом перепродал, как у нас сейчас это принято, а это большая политика в сфере продаж&quot;.</br></br>&quot;Вы активный участник той комиссии, которая изучала ситуацию на калийном рынке вообще, Вы анализировали работу \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Белорусской калийной компании\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              ", которая занимается реализацией калийных удобрений \n",
              "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Белоруссии\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
              "</mark>\n",
              " и \n",
              "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Российской Федерации\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
              "</mark>\n",
              ". Более того, Вы хорошо знаете ситуацию не только у нашего производителя, но и российского. И, наверно, я так полагаю, таких специалистов, как Вы, в стране немного, которые так владели бы ситуацией по продажам калийных удобрений&quot;, - констатировал \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Лукашенко\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
              "</mark>\n",
              ".</br></br>&quot;\n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Валерия Иванова\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
              "</mark>\n",
              " мы переведем на другую работу. У нас таких больших, вроде, и претензий к нему нет, но ему тяжело работать в этой сфере, поскольку он с ней никогда не сталкивался, - сообщил \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Лукашенко\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
              "</mark>\n",
              ". - Поэтому мы его опыт будем использовать здесь, внутри страны, он будет назначен на другую должность&quot;.</br></br>&quot;Нам ни в коем случае нельзя потерять российскую компанию, потому что мы мощнее, мы самая мощная компания по реализации калийных удобрений, если мы с россиянами действуем заодно. Но, естественно, не надо забывать и белорусские интересы, потому что с той стороны - частная компания. И у государства, и частника не всегда совпадают в нюансах цели, но, тем не менее, как они, так и мы, хотим продать побольше и подороже, это естественно, и в этом главном наши интересы совпадают, - отметил \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Лукашенко\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
              "</mark>\n",
              ". - Надо взять под контроль все потоки, по каким бы линиям они не реализовывались, и посмотреть на цены&quot;.</br></br>\n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Лукашенко\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
              "</mark>\n",
              " поручил серьезно проанализировать ситуацию в коллективе \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    БКК\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              ". Какие проблемы вызывали обеспокоенность \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Лукашенко\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
              "</mark>\n",
              ", в пресс-службе не уточнили.</br></br>Отметим, накануне, 22 июля на совещании по вопросам реализации калийных удобрений \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Лукашенко\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
              "</mark>\n",
              " заявил о необходимости усиливать белорусско-российское взаимодействие в калийной отрасли. Он напомнил, что в мае была создана совместная белорусско-российская рабочая группа, отметив: &quot;Поскольку были некоторые проблемы на нынешнем этапе, несогласованные действия и мнения по работе в данной сфере, и это усложнялось тем, что мы ведь не одни работали при реализации калийных удобрений, мы тесно сотрудничали с россиянами в этой сфере, поэтому было решение - привлечь, как принято сейчас говорить, независимых специалистов и проанализировать ситуацию&quot;. Он обратил внимание на то, что \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    БКК\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " была создана для повышения эффективности работы как белорусского госмонополиста &quot;\n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Беларуськалия\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              "&quot;, так и российского &quot;\n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Уралкалия\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              "&quot;.</br></br>\n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Лукашенко\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
              "</mark>\n",
              " потребовал незамедлительно приступить к выполнению договоренностей, достигнутых с российской стороной по увеличению эффективности продаж калийных удобрений в рамках БКК. Об этих договорённостях в сообщении президентской пресс-службы было сказано: &quot;Стороны пришли к соглашению, что финансирование деятельности \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Белорусской калийной компании\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " будет осуществляться в паритете 50/50. Кроме того, российская сторона согласилась усилить роль белорусских специалистов в управлении \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    БКК\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              ". Главе государства также доложено о том, что стороны нашли понимание по проблемным вопросам, которые стояли на повестке дня работы \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    БКК\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              ", и не позднее 1 апреля 2014 года будут приняты все решения, которые необходимы для повышения эффективности деятельности \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    БКК\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              "&quot;.</br></br>Еще ранее, 27 мая, в \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Администрации президента\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " \n",
              "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Белоруссии\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
              "</mark>\n",
              " состоялось совещание с рабочей группой по вопросу реализации калийных удобрений российского \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    ОАО &quot;Уралкалий&quot;\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " и белорусского \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    ОАО &quot;Беларуськалий&quot;\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " через \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    ЗАО &quot;Белорусская калийная компания&quot;\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              ". В ходе совещания \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Лукашенко\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
              "</mark>\n",
              " отметил, что он против &quot;многоканальности&quot; экспорта калийных удобрений, отметив: &quot;Но я сторонник все-таки одного мощного канала, где будет абсолютный паритет интересов и где интересы наши - белорусские, ни в коем случае не будут ущемлены. Об этом мы договорились с руководителем &quot;\n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Уралкалия\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              "&quot;. \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Лукашенко\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
              "</mark>\n",
              " напомнил, что \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    ЗАО &quot;Белорусская калийная компания&quot;\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " в своё время была создана по его прямому указанию. Он также сообщил о создании спецкомиссии, призванной оценить перспективы белорусско-российского сотрудничества в совместном сбыте калийных удобрений на мировом рынке. Председателем комиссии назначен глава правления белорусского государственного банка \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    ОАО &quot;Банк развития Республики Беларусь&quot;\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Сергей Румас\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
              "</mark>\n",
              ", контролировать работу комиссии \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Лукашенко\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
              "</mark>\n",
              " назначил председателя \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    КГБ\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " \n",
              "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Белоруссии\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
              "</mark>\n",
              " \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Валерия Вакульчика\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
              "</mark>\n",
              " и своего помощника по национальной безопасности - старшего сына \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Виктора Лукашенко\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
              "</mark>\n",
              ".</br></br>До этого, 28 декабря 2012 года \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Лукашенко\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
              "</mark>\n",
              " обсудил будущее &quot;\n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    БКК\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              "&quot; с российским миллиардером и сенатором \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Сулейманом Керимовым\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
              "</mark>\n",
              ". Согласно официальному сообщению пресс-службы президента \n",
              "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Белоруссии\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
              "</mark>\n",
              ", &quot;\n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Лукашенко\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
              "</mark>\n",
              " подтвердил приверженность заявленной ранее позиции, направленной на дальнейшее развитие компании, усиление ее влияния&quot;, а \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Керимов\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
              "</mark>\n",
              " &quot;отметил, что настроен на продолжение совместной работы&quot;. После этого, 20 мая \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Лукашенко\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
              "</mark>\n",
              " снова встретился с \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Керимовым\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
              "</mark>\n",
              " в \n",
              "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Минске\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
              "</mark>\n",
              " и обсудил с ним перспективы сотрудничества \n",
              "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Белоруссии\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
              "</mark>\n",
              " и \n",
              "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    России\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
              "</mark>\n",
              " при экспорте калийных удобрений. &quot;Любая попытка действовать врозь, особенно сейчас, ведь мы создали единую компанию по реализации калийных удобрений, будет только вредить и &quot;\n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Уралкалию\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              "&quot; с одной стороны, и &quot;\n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Беларуськалию\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              "&quot; с другой, - заявил \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Лукашенко\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
              "</mark>\n",
              ". - И мы не скрываем, что будем усиливать наши действия по единству на рынке калийных удобрений&quot;.</br></br>Как сообщало \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    ИА REGNUM\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              ", доля \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    БКК\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " на мировом рынке калийных удобрений оценивается в 43%. Преемником &quot;\n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    БКК\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              "&quot; должна стать создаваемая компания &quot;\n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Союзкалий\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              "&quot; с офисом в \n",
              "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Швейцарии\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
              "</mark>\n",
              ", начало её работы было запланировано на февраль-март 2013 года. До \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    БКК\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " российские и белорусские калийные удобрения реализовывались через московскую &quot;\n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Международную калийную компанию\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              "&quot;. Её сменила \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    БКК\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " (учреждена весной 2005 года), а на смену ей весной 2013 г. должна была прийти \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Soyuzkali GmbH\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " (&quot;Союзкалий&quot;) с офисом в \n",
              "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Швейцарии\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
              "</mark>\n",
              ". Однако представители \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Совмина\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
              "</mark>\n",
              " \n",
              "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Белоруссии\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
              "</mark>\n",
              " и &quot;\n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Уралкалия\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              "&quot; не смогли договориться об условиях сотрудничества и начало работы новой компании было перенесено на весну 2014 г. Тем временем &quot;Уралкалий&quot; нарастил объём экспорта своей продукции через дочерние компании, продемонстрировав отсутствие заинтересованности к продажам через \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    БКК\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              ".</br></div></span>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "nlp = spacy.load('ru_core_news_lg')\n",
        "doc = nlp(dataset_for_nltk[0][0])\n",
        "displacy.render(doc, jupyter=True, style='ent')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m deeppavlov install squad_bert\n",
        "\n",
        "!python -m deeppavlov install ner_ontonotes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dwac4QI2tHdg",
        "outputId": "68470a41-7c7b-43a5-c3eb-7ecfad54e84a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch<1.14.0,>=1.6.0\n",
            "  Downloading torch-1.13.1-cp310-cp310-manylinux1_x86_64.whl (887.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.5/887.5 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch<1.14.0,>=1.6.0) (4.7.1)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch<1.14.0,>=1.6.0)\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch<1.14.0,>=1.6.0)\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch<1.14.0,>=1.6.0)\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch<1.14.0,>=1.6.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch<1.14.0,>=1.6.0) (67.7.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch<1.14.0,>=1.6.0) (0.41.0)\n",
            "Installing collected packages: nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, nvidia-cudnn-cu11, torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.0.1+cu118\n",
            "    Uninstalling torch-2.0.1+cu118:\n",
            "      Successfully uninstalled torch-2.0.1+cu118\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.0.2+cu118 requires torch==2.0.1, but you have torch 1.13.1 which is incompatible.\n",
            "torchdata 0.6.1 requires torch==2.0.1, but you have torch 1.13.1 which is incompatible.\n",
            "torchtext 0.15.2 requires torch==2.0.1, but you have torch 1.13.1 which is incompatible.\n",
            "torchvision 0.15.2+cu118 requires torch==2.0.1, but you have torch 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 torch-1.13.1\n",
            "Collecting transformers<4.25.0,>=4.13.0\n",
            "  Downloading transformers-4.24.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers<4.25.0,>=4.13.0) (3.9.1)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0 (from transformers<4.25.0,>=4.13.0)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<4.25.0,>=4.13.0) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers<4.25.0,>=4.13.0) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers<4.25.0,>=4.13.0) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<4.25.0,>=4.13.0) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers<4.25.0,>=4.13.0) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers<4.25.0,>=4.13.0)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m78.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers<4.25.0,>=4.13.0) (4.64.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers<4.25.0,>=4.13.0) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers<4.25.0,>=4.13.0) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<4.25.0,>=4.13.0) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<4.25.0,>=4.13.0) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<4.25.0,>=4.13.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<4.25.0,>=4.13.0) (3.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.16.4 tokenizers-0.13.3 transformers-4.24.0\n",
            "2023-08-04 13:23:25.214 WARNING in 'deeppavlov.core.common.file'['file'] at line 39: \u001b[31;20m\n",
            "\n",
            "################################################################################\n",
            "# The model 'ner_ontonotes' has been removed from the DeepPavlov configs.\n",
            "# The model 'ner_ontonotes_bert' is used instead.\n",
            "# To disable this message please switch to 'ner_ontonotes_bert'.\n",
            "# Automatic name resolving will be disabled in the deeppavlov 1.2.0,\n",
            "# and if you try to use 'ner_ontonotes' you will get an ERROR.\n",
            "################################################################################\u001b[0m\n",
            "\n",
            "Collecting pytorch-crf==0.7.*\n",
            "  Downloading pytorch_crf-0.7.2-py3-none-any.whl (9.5 kB)\n",
            "Installing collected packages: pytorch-crf\n",
            "Successfully installed pytorch-crf-0.7.2\n",
            "Requirement already satisfied: transformers<4.25.0,>=4.13.0 in /usr/local/lib/python3.10/dist-packages (4.24.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers<4.25.0,>=4.13.0) (3.9.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from transformers<4.25.0,>=4.13.0) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<4.25.0,>=4.13.0) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers<4.25.0,>=4.13.0) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers<4.25.0,>=4.13.0) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<4.25.0,>=4.13.0) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers<4.25.0,>=4.13.0) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers<4.25.0,>=4.13.0) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers<4.25.0,>=4.13.0) (4.64.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers<4.25.0,>=4.13.0) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers<4.25.0,>=4.13.0) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<4.25.0,>=4.13.0) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<4.25.0,>=4.13.0) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<4.25.0,>=4.13.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<4.25.0,>=4.13.0) (3.4)\n",
            "Requirement already satisfied: torch<1.14.0,>=1.6.0 in /usr/local/lib/python3.10/dist-packages (1.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch<1.14.0,>=1.6.0) (4.7.1)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch<1.14.0,>=1.6.0) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch<1.14.0,>=1.6.0) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch<1.14.0,>=1.6.0) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch<1.14.0,>=1.6.0) (11.7.99)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch<1.14.0,>=1.6.0) (67.7.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch<1.14.0,>=1.6.0) (0.41.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "deeppavlov_ner = build_model(configs.ner, download=True)\n",
        "rus_document = dataset_for_nltk[0][0]\n",
        "deeppavlov_ner(rus_document)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "Xks-NWkCsebn",
        "outputId": "ccb32315-924d-4a2e-e148-8e1608b9ea12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-09f35b692997>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdeeppavlov_ner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfigs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mrus_document\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_for_nltk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdeeppavlov_ner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrus_document\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/deeppavlov/core/commands/infer.py\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m(config, mode, load_trained, install, download)\u001b[0m\n\u001b[1;32m     32\u001b[0m                 load_trained: bool = False, install: bool = False, download: bool = False) -> Chainer:\n\u001b[1;32m     33\u001b[0m     \u001b[0;34m\"\"\"Build and return the model described in corresponding configuration file.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minstall\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/deeppavlov/core/commands/utils.py\u001b[0m in \u001b[0;36mparse_config\u001b[0;34m(config, overwrite)\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0m_overwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m     \u001b[0mupdated_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_update_requirements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0mvariables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables_exact\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_variables_from_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdated_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/deeppavlov/core/commands/utils.py\u001b[0m in \u001b[0;36m_update_requirements\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcomponent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcomponents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mrequirements\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequirements_registry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0mrequirements\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'metadata'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'requirements'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'metadata'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'metadata'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Struct' object has no attribute 'get'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ],
      "metadata": {
        "id": "kKI3NHsfwOqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!locale"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvRoUgXXwV7N",
        "outputId": "b8272832-53ed-42a2-e61c-a8d390c1dbc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LANG=en_US.UTF-8\n",
            "LANGUAGE=\n",
            "LC_CTYPE=\"en_US.UTF-8\"\n",
            "LC_NUMERIC=\"en_US.UTF-8\"\n",
            "LC_TIME=\"en_US.UTF-8\"\n",
            "LC_COLLATE=\"en_US.UTF-8\"\n",
            "LC_MONETARY=\"en_US.UTF-8\"\n",
            "LC_MESSAGES=\"en_US.UTF-8\"\n",
            "LC_PAPER=\"en_US.UTF-8\"\n",
            "LC_NAME=\"en_US.UTF-8\"\n",
            "LC_ADDRESS=\"en_US.UTF-8\"\n",
            "LC_TELEPHONE=\"en_US.UTF-8\"\n",
            "LC_MEASUREMENT=\"en_US.UTF-8\"\n",
            "LC_IDENTIFICATION=\"en_US.UTF-8\"\n",
            "LC_ALL=\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZLw99t0gcnkl"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
        "\n",
        "\n",
        "import ru_core_news_lg\n",
        "\n",
        "import corus\n",
        "from corus import load_ne5\n",
        "from razdel import tokenize\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D, GlobalMaxPooling1D, Conv1D, GRU, LSTM, Dropout, Input\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "from sklearn import model_selection, preprocessing, linear_model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rQzSVUAYWsk"
      },
      "outputs": [],
      "source": [
        "def load_text_patched(path):\n",
        "    # do not convert \\r\\n to \\n\n",
        "    with open(path, newline='', encoding='utf-8') as file:\n",
        "        return file.read()\n",
        "\n",
        "dir = 'datasets/Collection5/'\n",
        "# load_ne5 do not accept encoding, but Colab sometimes fails to default to utf-8 for unknown reasons\n",
        "corus.ne5.load_text = load_text_patched\n",
        "records = load_ne5(dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CfAXxQksYrOD"
      },
      "outputs": [],
      "source": [
        "words_docs = []\n",
        "for ix, rec in enumerate(records):\n",
        "    words = []\n",
        "    for token in tokenize(rec.text):\n",
        "        type_ent = 'OUT'\n",
        "        for ent in rec.spans:\n",
        "            if (token.start >= ent.start) and (token.stop <= ent.stop):\n",
        "                type_ent = ent.type\n",
        "                break\n",
        "        words.append([token.text, type_ent])\n",
        "    words_docs.extend(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bN2HQbtPYtz8"
      },
      "outputs": [],
      "source": [
        "df_words = pd.DataFrame(words_docs, columns=['word', 'tag'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oiApiWC5Y2NV"
      },
      "outputs": [],
      "source": [
        "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(df_words['word'], df_words['tag'])\n",
        "\n",
        "# labelEncode целевую переменную\n",
        "encoder = preprocessing.LabelEncoder()\n",
        "train_y = encoder.fit_transform(train_y)\n",
        "valid_y = encoder.fit_transform(valid_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sKvMY9MOY6A9"
      },
      "outputs": [],
      "source": [
        "train_data = tf.data.Dataset.from_tensor_slices((train_x, train_y))\n",
        "valid_data = tf.data.Dataset.from_tensor_slices((valid_x, valid_y))\n",
        "\n",
        "train_data = train_data.batch(16)\n",
        "valid_data = valid_data.batch(16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2bCZ0MDY8Hd"
      },
      "outputs": [],
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_data = train_data.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "valid_data = valid_data.cache().prefetch(buffer_size=AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fz_B9tMtY-hj"
      },
      "outputs": [],
      "source": [
        "def custom_standardization(input_data):\n",
        "    return input_data\n",
        "\n",
        "vocab_size = 30000\n",
        "seq_len = 128\n",
        "\n",
        "vectorize_layer = TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode='int',\n",
        "    # ngrams=(1, 5),\n",
        "    output_sequence_length=seq_len)\n",
        "\n",
        "# Make a text-only dataset (no labels) and call adapt to build the vocabulary.\n",
        "text_data = train_data.map(lambda x, y: x)\n",
        "vectorize_layer.adapt(text_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5U8s3Dl1ZEWd"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 128\n",
        "\n",
        "class modelNER(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(modelNER, self).__init__()\n",
        "        self.emb = Embedding(vocab_size, embedding_dim)\n",
        "        self.gPool = GlobalMaxPooling1D()\n",
        "        self.fc1 = Dense(300, activation='relu')\n",
        "        self.fc2 = Dense(50, activation='relu')\n",
        "        self.fc3 = Dense(6, activation='softmax')\n",
        "\n",
        "    def call(self, x):\n",
        "        x = vectorize_layer(x)\n",
        "        x = self.emb(x)\n",
        "        pool_x = self.gPool(x)\n",
        "\n",
        "        fc_x = self.fc1(pool_x)\n",
        "        fc_x = self.fc2(fc_x)\n",
        "\n",
        "        concat_x = tf.concat([pool_x, fc_x], axis=1)\n",
        "        prob = self.fc3(concat_x)\n",
        "        return prob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rf13IgCRZGRE"
      },
      "outputs": [],
      "source": [
        "model = modelNER()\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "              metrics=['acc'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYzPj3cIZJdt",
        "outputId": "d28a4d53-dfa3-41b0-e784-5fc6db539b04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "12444/12444 [==============================] - 99s 7ms/step - loss: 0.2853 - acc: 0.9168 - val_loss: 0.2058 - val_acc: 0.9391\n",
            "Epoch 2/5\n",
            "12444/12444 [==============================] - 95s 8ms/step - loss: 0.1209 - acc: 0.9633 - val_loss: 0.2245 - val_acc: 0.9412\n",
            "Epoch 3/5\n",
            "12444/12444 [==============================] - 86s 7ms/step - loss: 0.1069 - acc: 0.9659 - val_loss: 0.2563 - val_acc: 0.8848\n",
            "Epoch 4/5\n",
            "12444/12444 [==============================] - 96s 8ms/step - loss: 0.1023 - acc: 0.9668 - val_loss: 0.2514 - val_acc: 0.8856\n",
            "Epoch 5/5\n",
            "12444/12444 [==============================] - 86s 7ms/step - loss: 0.0991 - acc: 0.9673 - val_loss: 0.2822 - val_acc: 0.8855\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f862f50ef20>"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "model.fit(train_data, validation_data=valid_data, epochs=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29MfhpQXctkR",
        "outputId": "f0749dc7-2ae3-4a6d-e2d4-aa8ef154f3b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2074/2074 [==============================] - 6s 3ms/step\n",
            "Precision score 0.811798884450888\n",
            "Recall score 0.7916335073535307\n",
            "F1 score 0.7852530264378447\n"
          ]
        }
      ],
      "source": [
        "y_pred1 = model.predict(valid_x)\n",
        "y_pred = np.argmax(y_pred1, axis=1)\n",
        "\n",
        "print(f'Precision score {precision_score(valid_y, y_pred , average=\"macro\")}')\n",
        "print(f'Recall score {recall_score(valid_y, y_pred , average=\"macro\")}')\n",
        "print(f'F1 score {f1_score(valid_y, y_pred , average=\"macro\")}')"
      ]
    }
  ]
}